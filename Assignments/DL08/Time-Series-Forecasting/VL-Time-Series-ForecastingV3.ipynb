{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting\n",
    "\n",
    "inspired by:\n",
    "- Jason Brownlee's Deep Learning for Time Series Forecasting Book, \n",
    "- Udacity Course Intro to TensorFlow for Deep Learning Lesson 8: Time Series Forecasting\n",
    "- Siraj Raval\n",
    "\n",
    "\n",
    "(written by Nicolaj C. Stache and Andreas F. Schneider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do we find time series data in the real world?\n",
    "\n",
    "There are many examples, just a few listed here:\n",
    "\n",
    "Weather Forecasts           |  Stock Prices\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"Bilder/weather.JPG\" alt=\"Stock\" style=\"height: 250px;\"/>  |  <img src=\"Bilder/stock.JPG\" alt=\"Stock\" style=\"height: 250px;\"/>\n",
    "\n",
    "Historical Trends           |  Electrical Demand\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"Bilder/moore.JPG\" alt=\"Stock\" style=\"height: 250px;\"/>  |  <img src=\"Bilder/elec_demand.JPG\" alt=\"Stock\" style=\"height: 250px;\"/>\n",
    "\n",
    "CO2 Concentration vs. Temperature         |\n",
    ":-------------------------:|\n",
    "<img src=\"Bilder/co2.JPG\" alt=\"Stock\" style=\"height: 400px;\"/> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Image Source](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a time series exactly?\n",
    "-  It's an ordered sequence of values usually equally spaced over time like \"every year\", \"every month\", \"every second\", and so on.\n",
    "\n",
    "- If you observe just one value than it is a univariate time series \n",
    "(e.g. Temperature)\n",
    "<img src=\"Bilder/weather.JPG\" alt=\"Stock\" style=\"height: 250px;\"/>\n",
    "\n",
    "- If you observe more than one it is a multivariate time series (e.g. Brainwaves, GPS coordinates or a Stereo Audio signal)\n",
    "<img src=\"Bilder/GPS.jpg\" alt=\"GPS\" width=\"500\"/>\n",
    "<img src=\"Bilder/stereo.JPG\" alt=\"Stereo\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Application Examples\n",
    "- You could **forecast** stock prices to become rich, or the temperature, or how many products you should produce to meet the demand in the future.\n",
    "- You could **understand the underlying process** that generated the time series e.g. study brainwaves to better understand sleep cycles.\n",
    "- Time series analysis can also be used to **detect anomalies** e.g. the traffic of an e-mail server in order to find out abnormal activity such as a server attack.\n",
    "\n",
    "##### IN THIS NOTEBOOK WE WILL FOCUS ON FORECASTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Patterns\n",
    "\n",
    "### Trend\n",
    "Values that gradually drift up or down, also known as upward or downward trend.\n",
    "\n",
    "> **For Example:**\n",
    "- CO2 concentration in the last decade -> **Upward trend**\n",
    "- drinking water reserves -> **Downward trend**\n",
    "- meat consumption worldwide-> **Upward trend**\n",
    "\n",
    "### Seasonality\n",
    "Patterns that repeat at predictable intervals and particular peaks and troughs. \n",
    "\n",
    "> **For Example:**\n",
    "- Temperature drops every winter in comparision to the summer\n",
    "- The same happens every night in comparison to the day.\n",
    "- Rush Hour\n",
    "- Sales before Christmas\n",
    "- ...\n",
    "\n",
    "\n",
    "### Noise\n",
    "This is completly unpredictable. Almost all data coming from sensors do have noise. Often times it is a noise with a Gaussian distribution. We usually want to forecast the mean of the noise.\n",
    "\n",
    "\n",
    "Trend         |  Seasonality        |  Noise\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src=\"Bilder/trend.JPG\" alt=\"Stock\" style=\"height: 200px;\"/>  |  <img src=\"Bilder/seasonality.JPG\" alt=\"Stock\" style=\"height: 200px;\"/> | <img src=\"Bilder/noise.JPG\" alt=\"Stock\" style=\"height: 200px;\"/> \n",
    "\n",
    "Trend + Seasonality + Noise |\n",
    ":-------------------------: |\n",
    "In many scenarios we will find all three of them together\n",
    "<img src=\"Bilder/trend-seasonality-noise.JPG\" alt=\"Stock\" style=\"height: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data partitioning for time series data\n",
    "\n",
    "<h1>Fixed Partioning</h1>       |  <h1>Roll-Forward Partioning</h1> \n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"Bilder/fixed-part.JPG\" alt=\"Stock\" style=\"height: 250px;\" align=\"left\"/>|  <img src=\"Bilder/roll-forward-part.JPG\" alt=\"Stock\" style=\"height: 250px;\"/> \n",
    "\n",
    "##### Fixed Partioning\n",
    "\n",
    "As shown above, we have a training, validation and test set. Since the data has obvioulsy a seasonality, we need to make sure to have always the same amount of each season. E.g. 1 year instead of 1.5 years, because otherwise some months would be more represented than others. We train the model on the validation period, if we are happy with the performance, we train the model with the training and validation period together. After that we evaluate it on the test period in order to check the performance as close to reality as possible. Lastly, against normal machine learning principles, we use the test set to train one last time before using the model in production. This is important, because the last data points of a data set is normally most important and relevant for the future.\n",
    "\n",
    "##### Roll-Forward Partioning\n",
    "\n",
    "As shown above, we move the green window to the right for each training iteration. With each iteration, we look further in the future with less data from the past. We treat the validation period as the \"future\" and use the training period to fit the model on this \"future\". Nevertheless, it is also common to keep all training data and just extend the green window within the validation data set. This means here to just extend the window to the right.\n",
    "\n",
    "**Advantage 1:** We get the forecasts and errors of multiple examles such as one-step-ahead, two-step-ahead, and so on. At the end, we know how good and how far we can predict into the future. \n",
    "\n",
    " **Advantage 2:** We receive more detailed information about when the data works well for predicting the future. E.g. always in winter the prediction could be worse than in summer. This can be useful information.\n",
    " \n",
    "**Advantage 3:** When we deploy a model, the inference is most of the time roll-forward anyway. As soon as we get a new datapoint, we take a few old points and the newly arrived datapoint and try to predict the new future.\n",
    "\n",
    "[Source1: Galit Shmueli](https://youtu.be/3k7qk__5tDY)\n",
    "\n",
    "[Source2: Udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classcial Forecasting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Forcasting - The simplest approach.\n",
    "\n",
    "Naive forecasts are the most cost-effective forecasting model, and provide a benchmark against which more sophisticated models can be compared. This forecasting method is only suitable for time series data. Using the naive approach, forecasts are produced that are equal to the last observed value. This method works quite well for economic and financial time series, which often have patterns that are difficult to reliably and accurately predict. If the time series is believed to have seasonality, the seasonal naïve approach may be more appropriate where the forecasts are equal to the value from last season. In time series notation: \n",
    "\n",
    "$\\hat{y}_{t+1} = y_t$ \n",
    "\n",
    "[Source: Wikipedia](https://en.wikipedia.org/wiki/Forecasting#Na%C3%AFve_approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classic approaches:\n",
    "- Simple Average (SA)\n",
    "- Autoregression (AR)\n",
    "- Moving Average (MA)\n",
    "- Autoregressive Moving Average (ARMA)\n",
    "- Autoregressive Integrated Moving Average (ARIMA)\n",
    "- Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "- Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\n",
    "- Vector Autoregression (VAR)\n",
    "- Vector Autoregression Moving-Average (VARMA)\n",
    "- Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\n",
    "- Simple Exponential Smoothing (SES)\n",
    "- Holt Winter’s Exponential Smoothing (HWES)\n",
    "\n",
    "[Source: Siraj Raval](https://youtu.be/d4Sn6ny_5LI)\n",
    "\n",
    "[Source: MC.ai](https://mc.ai/11-classical-time-series-forecasting-methods-in-python-cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for evaluation of the model\n",
    "\n",
    "`errors = forecast - actual`\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "Use this, if you want to penalize large errors.\n",
    "> `mse = np.square(errors).mean()`\n",
    "\n",
    "**Mean Absolute Error (MAE)** (also called Mean Absolute Deviation (MAD)) \n",
    "\n",
    "Use this if the error should be proportional to the value\n",
    "> `mae = np.abs(errors).mean()`\n",
    "\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "Mean ratio between the absolute error and the absolute value --> this gives an idea of the size of the errors compared to the values.\n",
    "> `mape = np.abs(errors / x_valid).mean()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The next code cell generates a typical time series with trend, seasonality and noise with a fixed time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None, label=None):\n",
    "    plt.plot(time[start:end], series[start:end], format, label=label)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    if label:\n",
    "        plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    \n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def white_noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1)\n",
    "\n",
    "slope = 0.05\n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "\n",
    "noise_level = 5\n",
    "noise = white_noise(time, noise_level, seed=42)\n",
    "\n",
    "series += noise\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Forecasting\n",
    "Now we try the simplest forcasting method to get a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "naive_forecast = series[split_time - 1:-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid, start=0, end=150, label=\"Series\")\n",
    "plot_series(time_valid, naive_forecast, start=1, end=151, label=\"Forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs with the MAE metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average\n",
    "A little bit more sophisticated, but still classical without any deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_forecast(series, window_size):\n",
    "  \"\"\"Forecasts the mean of the last few values.\n",
    "     If window_size=1, then this is equivalent to naive forecast\n",
    "     This implementation is *much* faster than the previous one\"\"\"\n",
    "  mov = np.cumsum(series)\n",
    "  mov[window_size:] = mov[window_size:] - mov[:-window_size]\n",
    "  return mov[window_size - 1:-1] / window_size\n",
    "\n",
    "window_size = 30\n",
    "moving_avg = moving_average_forecast(series, window_size)[split_time - window_size:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid, label=\"Series\")\n",
    "plot_series(time_valid, moving_avg, label=\"Moving average {} days\".format(window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it is much worse. Through a method calld **Differencing** we can remove trend and seasonality. We just substract the value at time t and the value one season (year) earlier at time t-365. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_series = (series[365:] - series[:-365])\n",
    "diff_time = time[365:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(diff_time, diff_series, label=\"Series(t) – Series(t–365)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "diff_moving_avg = moving_average_forecast(diff_series, window_size)[split_time - 365 - window_size:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, diff_series[split_time - 365:], label=\"Series(t) – Series(t–365)\")\n",
    "plot_series(time_valid, diff_moving_avg, label=\"Moving Average of Diff\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add back the value at time t-365 in order to get back the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid, label=\"Series\")\n",
    "plot_series(time_valid, diff_moving_avg_plus_past, label=\"Forecasts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Little bit better than naive forcasting. The problem is that after adding the old values the noise comes back. To remove the noise again, we just apply the moving average again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-359], 11) + diff_moving_avg\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid, label=\"Series\")\n",
    "plot_series(time_valid, diff_moving_avg_plus_smooth_past, label=\"Forecasts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great improvement!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switching to Machine Learning Models\n",
    "So far so good. From now on we will focus on more recent technologies like MLPs, RNNs and CNNs.\n",
    "\n",
    "## Difference between MLPs, RNNs and CNNs\n",
    "\n",
    "### MLP\n",
    "MLP stands for Multilayer Perceptron and can consist of multiple layers. Within the context of this notebook we only use fully connected layers (dense layers). The easiest possible MLP model is one hidden Dense layer and one output Dense layer.\n",
    "\n",
    "More information about MLP: https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
    "\n",
    "### RNN\n",
    "<h1>RNN Architecture</h1>       |  <h1>RNN Layer</h1> \n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"Bilder/rnn-architecture.JPG\" alt=\"Stock\" style=\"height: 250px;\" align=\"left\"/>|  <img src=\"Bilder/rnn.JPG\" alt=\"Stock\" style=\"height: 250px;\"/> \n",
    "\n",
    "A RNN is a neural network which contains recurrent layers. A recurrent layer is a layer that can sequentially process a sequence of inputs. RNNs can take all sorts of sequences as inputs such as time series data or sentences. \n",
    "In the left picture you can see a simple RNN architecture with two recurrent layers and one dense layer, which serves as the output layer. You feed the network with a batch of sequences such as the windows from a time series and it outputs a batch of forecasts. The input shape is 3-dimensional: the first dimension represents the batch size, the second dimension represents the time steps and the third dimenion represents the dimensionality of the inputs at each time step (e.g. for univariate time series it would be 1).\n",
    "\n",
    "A recurrent layer is composed of a single memory cell which is used repeatedly to compute the outputs. A memory cell is basically a small neural network. It can be a simple dense vanilla RNN cell or a complex memory cell such as LSTM or GRU (more information https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)\n",
    "\n",
    "It's important to understand that a recurrent layer only contains a single cell. In the right image you can see multiple cells but it's actually the same cell that is reused multiple times by the layer. At each time step the memory cell takes the value of the input sequcence at that time step, starting with X0, X1... and then it produces the output for the current time step, starting with Y0, Y1...\n",
    "The memory cell itself also produces another output at each time step called a state vector, or sometimes a context vector, starting with H0, H1...\n",
    "As you can see this state vector is fed as an additional input to the memory cell at the next time step. This is actually why this is called a recurrent layer. Part of the output of the memory cell at one time step is fed back to itself at the next time step. The beauty of this architecture is that it can handle sequences of any length using a constant number of parameters.\n",
    "\n",
    "\n",
    "### CNN\n",
    "Convolutional Neural Networks are very successful in image processing tasks such as image classification. Recently they get more and more used in handling sequences.\n",
    "In image processing we use 2D-Convolutional-Layers whereas in handling sequences we use 1D-Convolutional-layers. They work exactly the same except the windows slides only over one dimension, typically the time axis instead of two dimensions such as height and width of an image.\n",
    "In the image below you can see a typical 1D-Convolutional-Layer with a filter (kernel) of size three. You can see how it is computed and activated by a ReLU function in the depicted equation.\n",
    "\n",
    "A RNN Layer has some memory as explained above whereas a CNN has no memory at all. Each output takes only a small window into consideration (here 3 time steps).\n",
    "This seems like a serious limitation, but once you stack multiple 1D-Convolutional-layers, the top layers end up indirectly seeing a large part of the input sequence. This means Convnets are able to capture also very long-term patterns.\n",
    "<img src=\"Bilder/convnet.JPG\" alt=\"Stock\" style=\"height: 350px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We are using Tensorflow 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "print(\"Tensorflow Version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience Functions\n",
    "In this notebook we will create different kinds of models. The method *plot_graph* will produce a nice looking graph of the model.\n",
    "\n",
    "Additionally we use the *plot_loss* function to reduce boilerplate code. We only need to hand over the history from the training process and receive a nice looking plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(model, pngName):    \n",
    "    return tf.keras.utils.plot_model(\n",
    "                model,\n",
    "                to_file=pngName,\n",
    "                show_shapes=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir='LR',\n",
    "                expand_nested=True,\n",
    "                dpi=96\n",
    "    )\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Loss'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Data Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major part in working with Time-Series-Data is the data preperation. \n",
    "\n",
    "Time series data requires preparation before it can be used to train a supervised learning model,\n",
    "such as an LSTM neural network. For example, a univariate time series is represented as a\n",
    "vector of observations:\n",
    "\n",
    "##### [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "A supervised learning algorithm requires that data is provided as a collection of samples,\n",
    "where each sample has an input component (X) and an output component (y).\n",
    "\n",
    "For a univariate time series problem where we are interested in one-step\n",
    "predictions, the observations at prior time steps, so-called lag observations, are used as input\n",
    "and the output is the observation at the current time step. For example, the above 9-step\n",
    "univariate series can be expressed as a supervised learning problem with three time steps for\n",
    "input and one step as output, as follows:\n",
    "\n",
    "| X     | y  |\n",
    "|------ |------|\n",
    "|[10,20,30]|[40]|\n",
    "|[20,30,40]|[50]|\n",
    "|[30,40,50]|[60]|\n",
    "|[...]|[...]|\n",
    "\n",
    "### The 3 dimensions of the input\n",
    "* **Samples**. One sequence is one sample. A batch is comprised of one or more samples.\n",
    "* **Time Steps**. One time step is one point of observation in the sample. One sample is comprised of multiple time steps.\n",
    "* **Features**. One feature is one observation at a time step. One time step is comprised of one or more features.\n",
    "\n",
    "### Data Preparation\n",
    "The following function will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out what we will get when we use this *split_sequence* function with our time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define univariate time series\n",
    "series = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "print(\"------Before----------\")\n",
    "print(\"Orginal Data:\\n\",series, \"\\nShape:\", series.shape)\n",
    "print()\n",
    "print(\"------After----------\")\n",
    "# Define the number of time steps\n",
    "n_steps = 3\n",
    "# Generate  data\n",
    "X, y = split_sequence(series, n_steps)\n",
    "print(\"Transformed Data:\")\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])\n",
    "print(\"Shape:\",X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, activation='relu', input_dim=n_steps))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "plot_graph(model, 'single-headed-MLP-model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction of the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = array([70.0, 80.0, 90.0])\n",
    "x_input = x_input.reshape((1,n_steps))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(\"Desired:\\t\", 100.0, \"\\nPredicted:\\t\", yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add another *dense layer* to improve the result. You can try to see how much this affects the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Data Example\n",
    "\n",
    "Now we have two variables as an Input and still one output variable (Single-Step Forecasting). \n",
    "\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input X1** - [10, 20, 30, 40,  50,    60,   70,   80,   90]\n",
    "\n",
    "**Input X2** - [15, 25, 35, 45,  55,    65,   75,   85,   95]\n",
    "\n",
    "**Output Y** - [25, 45, 65, 85, 105, 125, 145, 165, 185]\n",
    "\n",
    "**-------------------**\n",
    "\n",
    "After data preparation, the data should look like this:\n",
    "\n",
    "| X     | y  |\n",
    "|------ |------|\n",
    "|[ [10, 15], [20, 25], [30, 35] ]|[65]|\n",
    "|[ [20, 25], [30, 35], [40, 45] ]|[85]|\n",
    "|[ [30, 35], [40, 45], [50, 55] ]|[105]|\n",
    "|[...]|[...]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the updated split_sequences function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        #check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix,0:2], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data and convert them into training samples. \n",
    "The output shows *Stacked Data*, which is the input of the *split_sequences* function. *Training Data* is the output of that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "# Output\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "print(\"Stacked Data:\\n\\n\", dataset)\n",
    "print(\"Shape: \",  dataset.shape)\n",
    "print(\"-------------\")\n",
    "print(\"\\nTraining Data:\\n\")\n",
    "# Number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Single-Headed CNN\n",
    "This time we replace the MLP model with a simple CNN model. \n",
    "\n",
    "A one-dimensional CNN is a CNN model that has a convolutional hidden layer that operates over a 1D sequence. This is followed by a pooling layer whose job it is to distill the output of the convolutional layer to the most important elements. This is then followed by a dense layer that interprets the features extracted by the convolutional part of the model. A flatten layer is used between the convolutional layers and the dense layer to reduce the feature maps to a single one-dimensional vector. This network is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(64, 2, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "plot_graph(model, 'single-headed-model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training and plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X, y, epochs=500, verbose=0)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction of the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = array([[80.0, 85.0], [90.0, 95.0], [100.0, 105.0]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(\"Desired:\\t\", 205.0, \"\\nPredicted:\\t\", yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-headed CNN Model\n",
    "Each input series can be handled by\n",
    "a separate CNN and the output of each of these submodels can be combined before a prediction\n",
    "is made for the output sequence.\n",
    "\n",
    "It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled. For example, it allows you to configure each submodel differently for each input series, such as the number of filter maps and the kernel size. This type of model can be defined using the Tensorflow/Keras Functional API.\n",
    "Now, n_features is 1 instead of 2 as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3\n",
    "n_features = 1\n",
    "X1 = X[:, :, 0].reshape(X.shape[0], X.shape[1], n_features)\n",
    "X2 = X[:, :, 1].reshape(X.shape[0], X.shape[1], n_features)\n",
    "# first input model\n",
    "visible1 = Input(shape=(n_steps, n_features))\n",
    "cnn1 = Conv1D(64, 2, activation='relu')(visible1)\n",
    "cnn1 = MaxPooling1D()(cnn1)\n",
    "cnn1 = Flatten()(cnn1)\n",
    "# second input model\n",
    "visible2 = Input(shape=(n_steps, n_features))\n",
    "cnn2 = Conv1D(64, 2, activation='relu')(visible2)\n",
    "cnn2 = MaxPooling1D()(cnn2)\n",
    "cnn2 = Flatten()(cnn2)\n",
    "\n",
    "# merge input models\n",
    "merge = concatenate([cnn1, cnn2])\n",
    "dense = Dense(50, activation='relu')(merge)\n",
    "output = Dense(1)(dense)\n",
    "\n",
    "model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit([X1, X2], y, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[80.0, 85.0], [90.0, 95.0], [100.0, 105.0]])\n",
    "x1 = x_input[:, 0].reshape((1, n_steps, n_features))\n",
    "x2 = x_input[:, 1].reshape((1, n_steps, n_features))\n",
    "yhat = model.predict([x1, x2], verbose=0)\n",
    "print(\"Desired:\\t\", 205.0, \"\\nPredicted:\\t\", yhat[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to predict the sinus function\n",
    "We start by creating training data. In between $x=-7$ and $x=7$ we populate $71$ points in order to have an interval of $0.2$ (just for convenience). The resulting red dots is a sinus and the training data available for auf NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(2,suppress=True)\n",
    "in_array = np.linspace(-7, 7, 71) \n",
    "out_array = np.sin(in_array) \n",
    "print(in_array)\n",
    "# red for numpy.sin() \n",
    "plt.plot(in_array, out_array, color = 'red', marker = \"o\") \n",
    "plt.title(\"numpy.sin()\") \n",
    "plt.xlabel(\"X\") \n",
    "plt.ylabel(\"Y\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for splitting the data as in the previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypterparameter and Model\n",
    "We use a stacked LSTM with 200 neurons each. A single LSTM network would do the job as well. \n",
    "In this example we use the last 10 input values and try to predict 20 future sinus samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 10,20\n",
    "# split into samples\n",
    "X, y = split_sequence(out_array, n_steps_in, n_steps_out)\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "print(\"There are {} training samples available\".format(X.shape[0]))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(200, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Start Training\n",
    "\n",
    "With live preview in each epoch how good the system is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "\n",
    "for i in range(0,75):\n",
    "    model.fit(X, y, epochs=1, verbose=0)\n",
    "    # demonstrate prediction\n",
    "    x_input = array(out_array[-10:])\n",
    "    x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "\n",
    "    ax.clear()\n",
    "    ax.set(xlim=(-7.5, 12), ylim=(-1.2, 1.2))\n",
    "    ax.plot(in_array, out_array, color = 'red', marker = \"o\")\n",
    "    ax.plot(np.linspace(7.2, 7.2+0.2*n_steps_out, num=n_steps_out, endpoint=False),yhat[0], color = 'blue', marker = \"o\")\n",
    "    fig.canvas.draw()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fibonacci Sequence Example\n",
    "In this example we will try to learn the fibonacci sequence and predict new fibonacci samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fib = lambda n: n if n < 2 else fib(n-1) + fib(n-2)\n",
    "np.set_printoptions(precision=0, suppress=True)\n",
    "out_array = []\n",
    "print(\"Fibonacci Numbers\")\n",
    "for i in range(1, 31):\n",
    "    nr=fib(i)\n",
    "    out_array = np.append(out_array,nr)\n",
    "    print(i,nr, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        print(\"No. \", i,\"- Input: \",X[i],\"Output: \",y[i])\n",
    "    return array(X), array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "**n_steps_in** -> How many data points the NN will get in order to do a prediction.\n",
    "\n",
    "**n_steps_out** -> How many data points the NN should predict.\n",
    "\n",
    "**n_train_amount** -> How many data points are available? We produced 30 data points as you can see the first cell in this section.\n",
    "\n",
    "**n_features** -> How many features do we have for the prediciton? In this case it is just one: The Fibonacci Numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in, n_steps_out, n_train_amount, n_features = 2, 1, 30, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preperation - Training data\n",
    "In order get always a full training sample with 2 Input values, we need to cut away some incomplete data at the front and at the end. We will end up with a training set with 27 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_sequence(out_array[:n_train_amount], n_steps_in, n_steps_out)\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test 4 different Models and find out which one is best.\n",
    "\n",
    "Choose 1, 2, 3 or 4, execute the cell and scroll then down and execute the \"Start Training\" Block. Compare the results and decide which one performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model - LSTM - Plain and simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model - Stacked LSTM  - A little bit more sophisticated\n",
    "\n",
    "We can stack multiple LSTM Layers on top of each other.\n",
    "An LSTM Layer produces a 2-dim Output (this is the interpretation) but needs a 3-dim Input.\n",
    "This is addressed by the *return_sequences* flag, which is a Boolean and determines whether to return the last output in the output sequence, or the full sequence.\n",
    "\n",
    "Do you want to learn more about the Input and Output Shapes? Read here:\n",
    "\n",
    "https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model - Bidirectional LSTM\n",
    "This LSTM network is not only able to look into the past but also into the future. This helps often times understanding the context. The process is to process the inputs both from past to future and from future to past. Implemenation is as easy as wrapping your LSTM Layer with **Bidirectional()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features))))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model - CNN\n",
    "It is also possible to use a Convolutional Neural Network to predict the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(64, 1, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training\n",
    "Choose one of the models above and start training here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=200, verbose=0, batch_size=16)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=0)\n",
    "random_nr=np.random.randint(1,31-n_steps_out-n_steps_in-1)\n",
    "#random_nr=6\n",
    "print(\"Random ID \", random_nr)\n",
    "print(\"Input:\\t\\t\", out_array[random_nr:random_nr+n_steps_in])\n",
    "print(\"Ground Truth:\\t\", out_array[random_nr+n_steps_in:random_nr+n_steps_in+n_steps_out])\n",
    "x_input = array(out_array[random_nr:random_nr+n_steps_in])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(\"Prediction:\\t\", yhat[0])\n",
    "x=tf.keras.losses.MSE(out_array[random_nr+n_steps_in:random_nr+n_steps_in+n_steps_out],yhat[0])\n",
    "print(\"MSE: {0:.2f}\".format(x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further/deeper information about the concepts of Time Series Forecasting\n",
    "\n",
    "In this notebook we discuss data preparation based on simple time series data. Before we start we need to talk about different concepts regarding time series data. \n",
    "\n",
    "**What are the inputs and outputs for a forecast?**\n",
    "> **Inputs** - Historical data provided to the model in order to make a single forecast.<br/>\n",
    "  **Outputs** - Prediction or forecast for a future time step beyond the data provided as input.\n",
    "  \n",
    "> **Description** - Defining the inputs and outputs of the model forces you to think about what exactly is or may be required to make a forecast. You may not be able to be specific when it comes to input data. For example, you may not know whether one or multiple prior time steps are required to make a forecast.\n",
    "\n",
    "**What are the endogenous and exogenous variables?**\n",
    "\n",
    "> **Endogenous** - Input variables that are influenced by other variables in the system and on which the output variable depends.<br/>\n",
    "  **Exogenous** - Input variables that are not influenced by other variables in the system and on which the output variable depends.\n",
    "  \n",
    "> **Description** - Typically, a time series forecasting problem has endogenous variables (e.g. the output is a\n",
    "function of some number of prior time steps) and may or may not have exogenous variables. Often, exogenous variables are ignored given the strong focus on the time series. Explicitly thinking about both variable types may help to identify easily overlooked exogenous data or even engineered features that may improve the model.\n",
    "\n",
    "**Are you working on a regression or classification predictive modeling problem?**\n",
    "\n",
    "> **Regression** - Forecast a numerical quantity. E.g. a price, a count, a volume<br/>\n",
    "  **Classification** - Classify as one of two or more labels. E.g. car, bike, up, down, hot, cold\n",
    "  \n",
    "> **Description** - There is some flexibility between these types. For example, a regression problem can be reframed as classification and a classification problem can be reframed as regression. Some problems, like predicting an ordinal value, can be framed as either classification and regression. It is possible that a reframing of your time series  forecasting problem may simplify it.\n",
    "\n",
    "\n",
    "**Are the time series variables unstructured or structured?**\n",
    "\n",
    "> **Unstructured** - No obvious systematic time-dependent pattern in a time series variable.<br/>\n",
    "  **Structered** - Systematic time-dependent patterns in a time series variable (e.g. trend and/or seasonality).\n",
    "  \n",
    "> **Description** - It is useful to plot each variable in a time series and inspect the plot looking for possible\n",
    "patterns. A time series for a single variable may not have any obvious pattern. We can think of a series with no pattern as unstructured, as in there is no discernible time-dependent structure. Alternately, a time series may have obvious patterns, such as a trend or seasonal cycles as structured. We can often simplify the modeling process by identifying and removing the obvious structures from the data, such as an increasing trend or repeating cycle. Some classical methods even allow you to specify parameters to handle these systematic structures directly.\n",
    "\n",
    "\n",
    "**Are you working on a univariate or multivariate time series problem?**\n",
    "\n",
    "> **Univariate** - One variable measured over time.<br/>\n",
    "  **Multivariate** - Multiple variables measured over time.<br/>\n",
    "  \n",
    "> **Description** - Considering this question with regard to inputs and outputs may add a further distinction.\n",
    "The number of variables may differ between the inputs and outputs, e.g. the data may not be\n",
    "symmetrical. For example, you may have multiple variables as input to the model and only be\n",
    "interested in predicting one of the variables as output. In this case, there is an assumption in\n",
    "the model that the multiple input variables aid and are required in predicting the single output\n",
    "variable.\n",
    "\n",
    "> **Univariate and Multivariate Inputs** - One or multiple input variables measured over time.<br/>\n",
    "  **Univariate and Multivariate Outputs** - One or multiple output variables to be predicted.\n",
    "\n",
    "\n",
    "**Do you require a single-step or a multi-step forecast?**\n",
    "\n",
    "> **single-step** - Forecast the next time step.<br/>\n",
    "  **multi-step** - Forecast more than one future time steps.\n",
    "  \n",
    "> **Description** - A forecast problem that requires a prediction of the next time step is called a one-step forecast\n",
    "model. Whereas a forecast problem that requires a prediction of more than one time step is called a multi-step forecast model. The more time steps to be projected into the future, the more challenging the problem given the compounding nature of the uncertainty on each forecasted time step.\n",
    "\n",
    "\n",
    "**Do you require a static or a dynamically updated model?**\n",
    "\n",
    "> **Static** - A forecast model is fit once and used to make predictions.<br/>\n",
    "  **Dynamic** - A forecast model is fit on newly available data prior to each prediction.\n",
    "\n",
    "**Are your observations contiguous or discontiguous?**\n",
    "\n",
    "> **Contiguous** - Observations are made uniform over time. E.g. each hour, day, month.<br/>\n",
    "  **Discontiguous** - Observations are not uniform over time. E.g. missing or corrupt values\n",
    "  \n",
    "> **Description** - In the case of non-uniform observations, specific data formatting may be required when fitting some models to make the observations uniform over time. E.g. interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
