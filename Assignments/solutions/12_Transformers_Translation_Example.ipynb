{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgNHSyc9-7BA"
   },
   "source": [
    "# Machine Translation using a Transformer\n",
    "This tutorial demonstrates building and training a English to Spanish [Transformer Model](https://arxiv.org/abs/1706.03762)  from a few parallel lines. Transformer model is currently a state-of-the-art machine translation system that uses self-attention that can learn both short-range and long-range relations and a Feedforward Neural Network that enables model parallelism. Comparing to the [Official TensorFlow Tutorial](https://www.tensorflow.org/alpha/tutorials/text/transformer), this model requires less understanding of TensorFlow specific functions and avoids reshaping Tensors.\n",
    "\n",
    "(This Notebook is based on https://github.com/LastRemote/Transformer-TF2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJsNN6j5-d3C"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "import numpy as np\n",
    "import unicodedata, re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda, Layer, Embedding, LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrMMHJ5TR9lM"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Let's start by using some parallel text examples in English and Spanish. In real life applications, the datasets are a lot bigger. GPT-3 for example is trained on about 500 billion tokens. But for demonstration purposes, a few sentences should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TYs8FSTRyvS"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  (\"Do you want a cup of coffee?\", \"¿Quieres una taza de café?\"),\n",
    "  (\"I've had coffee already.\", \"Ya tomé café.\"),\n",
    "  (\"Can I get you a coffee?\", \"¿Quieres que te traiga un café?\"),\n",
    "  (\"Please give me some coffee.\", \"Dame algo de café por favor.\"),\n",
    "  (\"Would you like me to make coffee?\", \"¿Quieres que prepare café?\"),\n",
    "  (\"Two coffees, please.\", \"Dos cafés, por favor.\"),\n",
    "  (\"How about a cup of coffee?\", \"¿Qué tal una taza de café?\"),\n",
    "  (\"I drank two cups of coffee.\", \"Me tomé dos tazas de café.\"),\n",
    "  (\"Would you like to have a cup of coffee?\", \"¿Te gustaría tomar una taza de café?\"),\n",
    "  (\"There'll be coffee and cake at five.\", \"A las cinco habrá café y un pastel.\"),\n",
    "  (\"Another coffee, please.\", \"Otro café, por favor.\"),\n",
    "  (\"I made coffee.\", \"Hice café.\"),\n",
    "  (\"I would like to have a cup of coffee.\", \"Quiero beber una taza de café.\"),\n",
    "  (\"Do you want me to make coffee?\", \"¿Quieres que haga café?\"),\n",
    "  (\"It is hard to wake up without a strong cup of coffee.\", \"Es difícil despertarse sin una taza de café fuerte.\"),\n",
    "  (\"All I drank was coffee.\", \"Todo lo que bebí fue café.\"),\n",
    "  (\"I've drunk way too much coffee today.\", \"He bebido demasiado café hoy.\"),\n",
    "  (\"Which do you prefer, tea or coffee?\", \"¿Qué prefieres, té o café?\"),\n",
    "  (\"There are many kinds of coffee.\", \"Hay muchas variedades de café.\"),\n",
    "  (\"I will make some coffee.\",\t\"Prepararé algo de café.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "689Pk6Hz6Mos"
   },
   "source": [
    "To prepare our sentences for the training of our transformer we need to regularize them. Therefore we add spaces arround punctuations, remove extra spaces and any special characters. Additionally we need to add \\<start> and \\<end> tokens to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "7xIAQHyhSHts",
    "outputId": "c0aedf70-307d-48fb-d934-283c3b379f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ('Do you want a cup of coffee?', '¿Quieres una taza de café?')\n",
      "Preprocessed: ('<start> Do you want a cup of coffee ? <end>', '<start> ¿ Quieres una taza de cafe ? <end>')\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = re.sub(r\"([?.!¡,¿])\", r\" \\1 \", s) # Add spaces around punctuations\n",
    "    s = re.sub(r'[\" \"]+', \" \", s) # Remove extra space\n",
    "    s = re.sub(r\"[^a-zA-Z?.!¡,¿áéíóú¡üñ]+\", \" \", s) # Remove other characters\n",
    "    s = s.strip()\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "print(\"Original:\", sentences[0])\n",
    "sentences = [(preprocess(en), preprocess(es)) for (en, es) in sentences]\n",
    "print(\"Preprocessed:\", sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-aq0zFg5tAQ"
   },
   "source": [
    "We then tokenize both source and target sentences into lists of integers, and pad zeros at the end of each sequence to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "g3bpAfYoSdTu",
    "outputId": "6d091ea9-f964-4a9e-d25c-dc99dcc0ad57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [1, 12, 8, 19, 9, 10, 6, 3, 7, 2]\n",
      "Padded: [ 1 12  8 19  9 10  6  3  7  2  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "source_sentences, target_sentences = list(zip(*sentences))\n",
    "\n",
    "# In this illustration, I choose not to specify num_words and oov_token due to the size of data.\n",
    "# for details, please visit https://keras.io/preprocessing/text/\n",
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') \n",
    "source_tokenizer.fit_on_texts(source_sentences)\n",
    "source_data = source_tokenizer.texts_to_sequences(source_sentences)\n",
    "print(\"Sequence:\", source_data[0])\n",
    "source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
    "print(\"Padded:\", source_data[0])\n",
    "\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_tokenizer.fit_on_texts(target_sentences)\n",
    "target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation models take the entire source sentence and an incomplete sentence in target language as inputs at once, and predict the next word for the incomplete sentence.\n",
    "We create labels for the decoder by shifting the target sequence one to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "Tnlm3gEtSsb_",
    "outputId": "79103aab-c0e0-4756-d46f-86d8351ecbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sequence [ 1  6 11  9 10  5  3  7  2  0  0  0]\n",
      "Target label [ 6. 11.  9. 10.  5.  3.  7.  2.  0.  0.  0.  0.]\n",
      "Size of source vocabulary:  65\n",
      "Size of target vocabulary:  60\n"
     ]
    }
   ],
   "source": [
    "target_labels = np.zeros(target_data.shape)\n",
    "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
    "\n",
    "print(\"Target sequence\", target_data[0])\n",
    "print(\"Target label\", target_labels[0])\n",
    "\n",
    "source_vocab_len = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_len = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Size of source vocabulary: \", source_vocab_len)\n",
    "print(\"Size of target vocabulary: \", target_vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-oKCaIAWOSX"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrfG2a42TB7F"
   },
   "source": [
    "# Transformer Structure\n",
    "Then we build the entire structure for transformer. It is actually not hard at all!\n",
    "\n",
    "In this cell we define our modelparameters, feel free to play with them around. Remember the more parameters you use, the better the accuracy of the transformer gets. However, it also quickly becomes impossible to train large transformers on a normal GPU because they no longer fit into the memory.\n",
    "\n",
    "For our few example sentences the defined parameters should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejOHRGKg-1Oh"
   },
   "outputs": [],
   "source": [
    "# Transformer parameters\n",
    "d_model = 64 # 512 in the original paper\n",
    "d_k = 16 # 64 in the original paper\n",
    "d_v = 16 # 64 in the original paper\n",
    "n_heads = 4 # 8 in the original paper\n",
    "n_encoder_layers = 2 # 6 in the original paper\n",
    "n_decoder_layers = 2 # 6 in the original paper\n",
    "\n",
    "max_token_length = 20 # 512 in the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyUQ0EU88dDk"
   },
   "source": [
    "## Transformer Attention\n",
    "\n",
    "First we will be working on the single head transformer attention mechanism. A single head attention takes 3 inputs as Query (q), Key (k) and Value (v), and it finds an unidirectional connection from each of the query words to each of the key words. In the transformer model key and value inputs are always the same.\n",
    "\n",
    "Each of query, key, value goes through a separate linear transform to a lower dimensionality to make the dimensionality of multi-headed attention to be smaller. Every linear layer in the Transformer model is using Xavier initialization ('glorot_uniform'). The output is then created by a rather simple mathematical equation:![Equation for Attention](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "\n",
    "Image source: Alammar, Jay (2018). The Illustrated Transformer. Retrieved from https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "If we are making a decoder self-attention, we have to be a little careful since the full decoding sentence is not available in practice and should be generated step by step. Therefore, we cannot assume future attentions from the query word and a key word that has not been generated. Since the Transformer model is always generating the next word given an incomplete sequence, we should remove the attention from the query word to any word appeared later, which is the strictly upper triangular region except the main diagonals in $ Q \\times K^T $ matrix. That is to set the strict upper triangle of $ Q \\times K^T $ to negative infinity (zero after softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqXGnAVw_ow3"
   },
   "outputs": [],
   "source": [
    "class SingleHeadAttention(Layer):\n",
    "    def __init__(self, input_shape=(3, -1, d_model), dropout=.0, masked=None):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.q = Dense(d_k, input_shape=(-1, d_model), kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "        self.normalize_q = Lambda(lambda x: x / np.sqrt(d_k))\n",
    "        self.k = Dense(d_k, input_shape=(-1, d_model), kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "        self.v = Dense(d_v, input_shape=(-1, d_model), kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "        self.dropout = dropout\n",
    "        self.masked = masked\n",
    "\n",
    "    # Inputs: [query, key, value]\n",
    "    def call(self, inputs, training=None):\n",
    "        assert len(inputs) == 3\n",
    "        # We use a lambda layer to divide vector q by sqrt(d_k) according to the equation\n",
    "        q = self.normalize_q(self.q(inputs[0]))\n",
    "        k = self.k(inputs[1])\n",
    "        \n",
    "        # The dimensionality of q is (batch_size, query_length, d_k) and that of k is (batch_size, key_length, d_k)\n",
    "        # So we will do a matrix multication by batch after transposing last 2 dimensions of k\n",
    "        # tf.shape(attn_weights) = (batch_size, query_length, key_length)\n",
    "        attn_weights = tf.matmul(q, tf.transpose(k, perm=[0,2,1]))\n",
    "        \n",
    "        if self.masked: # Prevent future attentions in decoding self-attention\n",
    "            # Create a matrix where the strict upper triangle (not including main diagonal) is filled with -inf and 0 elsewhere\n",
    "            length = tf.shape(attn_weights)[-1]\n",
    "            attn_mask = tf.fill((length, length), -np.inf)\n",
    "            attn_mask = tf.linalg.band_part(attn_mask, 0, -1) # Get upper triangle\n",
    "            attn_mask = tf.linalg.set_diag(attn_mask, tf.zeros((length))) # Set diagonal to zeros to avoid operations with infinity\n",
    "            # This matrix is added to the attention weights so all future attention will have -inf logits (0 after softmax)\n",
    "            attn_weights += attn_mask\n",
    "        \n",
    "        # Softmax along the last dimension\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        \n",
    "        if training: # Attention dropout included in the original paper. This is possibly to encourage multihead diversity.\n",
    "            attn_weights = tf.nn.dropout(attn_weights, rate=self.dropout)\n",
    "        v = self.v(inputs[2])\n",
    "        \n",
    "        return tf.matmul(attn_weights, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8We5UHPIuZm"
   },
   "source": [
    "Now let's use multiple single head attention and a linear layer to build a multihead attention. There is no need to reshape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiolVDYwFk00"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, dropout=.0, masked=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attn_heads = list()\n",
    "        for i in range(n_heads): \n",
    "            self.attn_heads.append(SingleHeadAttention(dropout=dropout, masked=masked))\n",
    "        self.linear = Dense(d_model, input_shape=(-1, n_heads * d_v), kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        attentions = [self.attn_heads[i](x, training=training) for i in range(n_heads)]\n",
    "        concatenated_attentions = tf.concat(attentions, axis=-1)\n",
    "        \n",
    "        return self.linear(concatenated_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzCW3dGJKvEJ"
   },
   "source": [
    "## Encoder and Decoder\n",
    "\n",
    "This is the flowchart for the whole transformer architecture, where the encoder is the block to the left, and decoder is the block to the right. Note that since the output shape of either encoder or decoder is the same as its corresponding input shape, both the encoder unit and the decoder unit can be stacked.\n",
    "![Transformer Architecture](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)\n",
    "\n",
    "We then present the transformer encoder architecture. Each encoder has a multihead self-attention (encoder-encoder) sublayer and a feedforward sublayer (two dense layers with ReLU activation in between). Each sublayer is followed by a LayerNorm taking the sublayer residually as follows:\n",
    "\n",
    "$$\\Large{\\mathit{LayerNorm}(x + \\mathit{sublayer}(x))} $$\n",
    "\n",
    "Dropout is applied after each sublayer before layer normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIyEIPm2sq9O"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, dropout=.1, attention_dropout=.0, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.dropout_rate = dropout\n",
    "        self.attention_dropout_rate = attention_dropout\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.multihead_attention = MultiHeadAttention(dropout=self.attention_dropout_rate)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.layer_normalization1 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.linear1 = Dense(input_shape[-1] * 4, input_shape=input_shape, activation='relu',\n",
    "                            kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.linear2 = Dense(input_shape[-1], input_shape=self.linear1.compute_output_shape(input_shape),\n",
    "                            kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.layer_normalization2 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "        super(TransformerEncoder, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        sublayer1 = self.multihead_attention((x, x, x), training=training)\n",
    "        sublayer1 = self.dropout1(sublayer1, training=training)\n",
    "        layernorm1 = self.layer_normalization1(x + sublayer1)\n",
    "\n",
    "        sublayer2 = self.linear2(self.linear1(layernorm1))\n",
    "        sublayer1 = self.dropout2(sublayer2, training=training)\n",
    "        layernorm2 = self.layer_normalization2(layernorm1 + sublayer2)\n",
    "        \n",
    "        return layernorm2\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEz1iDH-Y-UG"
   },
   "source": [
    "The decoder is constructed in the same fashion, except that there are three sublayers instead of two: a masked multihead self-attention layer (decoder-decoder), a multihead encoder attention layer (decoder-encoder) and a feedforward layer just like the one in an encoder unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6FC8ZLy8_-R"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, dropout=.0, attention_dropout=.0, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.dropout_rate = dropout\n",
    "        self.attention_dropout_rate = attention_dropout\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.multihead_self_attention = MultiHeadAttention(dropout=self.attention_dropout_rate, masked=True)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.layer_normalization1 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.multihead_encoder_attention = MultiHeadAttention(dropout=self.attention_dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.layer_normalization2 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.linear1 = Dense(input_shape[-1] * 4, input_shape=input_shape, activation='relu',\n",
    "                            kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.linear2 = Dense(input_shape[-1], input_shape=self.linear1.compute_output_shape(input_shape),\n",
    "                            kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.dropout3 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.layer_normalization3 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "        super(TransformerDecoder, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, hidden, training=None):\n",
    "        sublayer1 = self.multihead_self_attention((x, x, x))\n",
    "        sublayer1 = self.dropout1(sublayer1, training=training)\n",
    "        layernorm1 = self.layer_normalization1(x + sublayer1)\n",
    "\n",
    "        sublayer2 = self.multihead_encoder_attention((x, hidden, hidden))\n",
    "        sublayer2 = self.dropout2(sublayer2, training=training)\n",
    "        layernorm2 = self.layer_normalization2(layernorm1 + sublayer2)\n",
    "\n",
    "        sublayer3 = self.linear2(self.linear1(layernorm1))\n",
    "        sublayer3 = self.dropout3(sublayer3, training=training)\n",
    "        layernorm3 = self.layer_normalization2(layernorm2 + sublayer3)\n",
    "        \n",
    "        return layernorm3\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gd9sGHuA22m"
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In the original Transformer implementation, the researchers used a sinusoidal function to get positional encoding which will append to encoder and decoder word embeddings. This is to give information about the position of each token. The function looks as follows:\n",
    "\n",
    " $$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5ZOk2dowenm"
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(Layer): #Inspired from https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer_Torch.ipynb\n",
    "    def __init__(self):\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        self.sinusoidal_encoding = np.array([self.get_positional_angle(pos) for pos in range(max_token_length)], dtype=np.float32)\n",
    "        self.sinusoidal_encoding[:, 0::2] = np.sin(self.sinusoidal_encoding[:, 0::2])\n",
    "        self.sinusoidal_encoding[:, 1::2] = np.cos(self.sinusoidal_encoding[:, 1::2])\n",
    "        self.sinusoidal_encoding = tf.cast(self.sinusoidal_encoding, dtype=tf.float32) # Casting the array to Tensor for slicing\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x + self.sinusoidal_encoding[:tf.shape(x)[1]]\n",
    "        #return x + tf.slice(self.sinusoidal_encoding, [0, 0], [tf.shape(x)[1], d_model])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_angle(self, pos, dim):\n",
    "        return pos / np.power(10000, 2 * (dim // 2) / d_model)\n",
    "    \n",
    "    def get_positional_angle(self, pos):\n",
    "        return [self.get_angle(pos, dim) for dim in range(d_model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlhp-s1G60FQ"
   },
   "source": [
    "## Assembling the Full Architecture\n",
    "Now we can build the full architecture of transformer using positional encoding, encoder layers and decoder layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPyXChj3Xjet"
   },
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, dropout=.1, attention_dropout=.0, **kwargs):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "        self.encoding_embedding = Embedding(source_vocab_len, d_model)\n",
    "        self.decoding_embedding = Embedding(target_vocab_len, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding()\n",
    "        self.encoder = [TransformerEncoder(dropout=dropout, attention_dropout=attention_dropout) for i in range(n_encoder_layers)]\n",
    "        self.decoder = [TransformerDecoder(dropout=dropout, attention_dropout=attention_dropout) for i in range(n_decoder_layers)]\n",
    "        self.decoder_final = Dense(target_vocab_len, input_shape=(None, d_model))\n",
    "    \n",
    "    def call(self, inputs, training=None): # Source_sentence and decoder_input\n",
    "        source_sentence, decoder_input = inputs\n",
    "        embedded_source = self.encoding_embedding(source_sentence)\n",
    "        encoder_output = self.pos_encoding(embedded_source)\n",
    "        \n",
    "        for encoder_unit in self.encoder:\n",
    "            encoder_output = encoder_unit(encoder_output, training=training)\n",
    "\n",
    "        embedded_target = self.decoding_embedding(decoder_input)\n",
    "        decoder_output = self.pos_encoding(embedded_target)\n",
    "        \n",
    "        for decoder_unit in self.decoder:\n",
    "            decoder_output = decoder_unit(decoder_output, encoder_output, training=training)\n",
    "        \n",
    "        if training:\n",
    "            decoder_output = self.decoder_final(decoder_output)\n",
    "            decoder_output = tf.nn.softmax(decoder_output, axis=-1)\n",
    "        \n",
    "        else:\n",
    "            decoder_output = self.decoder_final(decoder_output[:, -1:, :])\n",
    "            decoder_output = tf.nn.softmax(decoder_output, axis=-1)\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wN3g869inv7e"
   },
   "source": [
    "# Training\n",
    "This model can be trained in two ways, either using TensorFlow GradientTape to update the model weights manually in a training function, or simply using Keras model.fit() method to start training. For simplicity reasons we will use the second method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11028
    },
    "colab_type": "code",
    "id": "TQS00n7DfcYZ",
    "outputId": "eae43f6f-ca59-435a-9ac4-b7f5c44d648b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4/4 - 5s - loss: 3.7038 - accuracy: 0.2458 - 5s/epoch - 1s/step\n",
      "Epoch 2/300\n",
      "4/4 - 0s - loss: 3.1218 - accuracy: 0.3542 - 51ms/epoch - 13ms/step\n",
      "Epoch 3/300\n",
      "4/4 - 0s - loss: 2.9477 - accuracy: 0.3542 - 51ms/epoch - 13ms/step\n",
      "Epoch 4/300\n",
      "4/4 - 0s - loss: 2.7782 - accuracy: 0.3542 - 49ms/epoch - 12ms/step\n",
      "Epoch 5/300\n",
      "4/4 - 0s - loss: 2.6084 - accuracy: 0.3792 - 50ms/epoch - 12ms/step\n",
      "Epoch 6/300\n",
      "4/4 - 0s - loss: 2.5050 - accuracy: 0.4042 - 51ms/epoch - 13ms/step\n",
      "Epoch 7/300\n",
      "4/4 - 0s - loss: 2.4205 - accuracy: 0.4250 - 51ms/epoch - 13ms/step\n",
      "Epoch 8/300\n",
      "4/4 - 0s - loss: 2.3785 - accuracy: 0.4417 - 50ms/epoch - 13ms/step\n",
      "Epoch 9/300\n",
      "4/4 - 0s - loss: 2.3218 - accuracy: 0.4417 - 51ms/epoch - 13ms/step\n",
      "Epoch 10/300\n",
      "4/4 - 0s - loss: 2.2814 - accuracy: 0.4250 - 54ms/epoch - 13ms/step\n",
      "Epoch 11/300\n",
      "4/4 - 0s - loss: 2.2880 - accuracy: 0.4000 - 53ms/epoch - 13ms/step\n",
      "Epoch 12/300\n",
      "4/4 - 0s - loss: 2.2130 - accuracy: 0.4458 - 51ms/epoch - 13ms/step\n",
      "Epoch 13/300\n",
      "4/4 - 0s - loss: 2.1701 - accuracy: 0.4625 - 51ms/epoch - 13ms/step\n",
      "Epoch 14/300\n",
      "4/4 - 0s - loss: 2.0876 - accuracy: 0.4583 - 53ms/epoch - 13ms/step\n",
      "Epoch 15/300\n",
      "4/4 - 0s - loss: 2.0468 - accuracy: 0.4708 - 53ms/epoch - 13ms/step\n",
      "Epoch 16/300\n",
      "4/4 - 0s - loss: 1.9671 - accuracy: 0.4875 - 51ms/epoch - 13ms/step\n",
      "Epoch 17/300\n",
      "4/4 - 0s - loss: 1.8896 - accuracy: 0.5167 - 52ms/epoch - 13ms/step\n",
      "Epoch 18/300\n",
      "4/4 - 0s - loss: 1.8612 - accuracy: 0.5500 - 60ms/epoch - 15ms/step\n",
      "Epoch 19/300\n",
      "4/4 - 0s - loss: 1.7762 - accuracy: 0.5375 - 51ms/epoch - 13ms/step\n",
      "Epoch 20/300\n",
      "4/4 - 0s - loss: 1.7719 - accuracy: 0.5250 - 52ms/epoch - 13ms/step\n",
      "Epoch 21/300\n",
      "4/4 - 0s - loss: 1.7382 - accuracy: 0.5583 - 54ms/epoch - 13ms/step\n",
      "Epoch 22/300\n",
      "4/4 - 0s - loss: 1.6927 - accuracy: 0.5625 - 50ms/epoch - 13ms/step\n",
      "Epoch 23/300\n",
      "4/4 - 0s - loss: 1.6016 - accuracy: 0.5792 - 51ms/epoch - 13ms/step\n",
      "Epoch 24/300\n",
      "4/4 - 0s - loss: 1.5719 - accuracy: 0.6000 - 51ms/epoch - 13ms/step\n",
      "Epoch 25/300\n",
      "4/4 - 0s - loss: 1.5267 - accuracy: 0.5958 - 54ms/epoch - 14ms/step\n",
      "Epoch 26/300\n",
      "4/4 - 0s - loss: 1.5110 - accuracy: 0.6292 - 52ms/epoch - 13ms/step\n",
      "Epoch 27/300\n",
      "4/4 - 0s - loss: 1.4694 - accuracy: 0.6333 - 52ms/epoch - 13ms/step\n",
      "Epoch 28/300\n",
      "4/4 - 0s - loss: 1.4299 - accuracy: 0.6375 - 51ms/epoch - 13ms/step\n",
      "Epoch 29/300\n",
      "4/4 - 0s - loss: 1.3788 - accuracy: 0.6333 - 53ms/epoch - 13ms/step\n",
      "Epoch 30/300\n",
      "4/4 - 0s - loss: 1.3318 - accuracy: 0.6458 - 51ms/epoch - 13ms/step\n",
      "Epoch 31/300\n",
      "4/4 - 0s - loss: 1.3219 - accuracy: 0.6458 - 51ms/epoch - 13ms/step\n",
      "Epoch 32/300\n",
      "4/4 - 0s - loss: 1.2589 - accuracy: 0.6583 - 53ms/epoch - 13ms/step\n",
      "Epoch 33/300\n",
      "4/4 - 0s - loss: 1.2394 - accuracy: 0.6750 - 53ms/epoch - 13ms/step\n",
      "Epoch 34/300\n",
      "4/4 - 0s - loss: 1.1970 - accuracy: 0.6833 - 52ms/epoch - 13ms/step\n",
      "Epoch 35/300\n",
      "4/4 - 0s - loss: 1.1604 - accuracy: 0.6750 - 50ms/epoch - 13ms/step\n",
      "Epoch 36/300\n",
      "4/4 - 0s - loss: 1.1434 - accuracy: 0.6750 - 50ms/epoch - 12ms/step\n",
      "Epoch 37/300\n",
      "4/4 - 0s - loss: 1.0915 - accuracy: 0.7125 - 52ms/epoch - 13ms/step\n",
      "Epoch 38/300\n",
      "4/4 - 0s - loss: 1.0340 - accuracy: 0.7333 - 53ms/epoch - 13ms/step\n",
      "Epoch 39/300\n",
      "4/4 - 0s - loss: 0.9837 - accuracy: 0.7375 - 51ms/epoch - 13ms/step\n",
      "Epoch 40/300\n",
      "4/4 - 0s - loss: 0.9946 - accuracy: 0.7250 - 53ms/epoch - 13ms/step\n",
      "Epoch 41/300\n",
      "4/4 - 0s - loss: 0.9449 - accuracy: 0.7500 - 57ms/epoch - 14ms/step\n",
      "Epoch 42/300\n",
      "4/4 - 0s - loss: 0.8943 - accuracy: 0.7417 - 52ms/epoch - 13ms/step\n",
      "Epoch 43/300\n",
      "4/4 - 0s - loss: 0.8736 - accuracy: 0.7667 - 51ms/epoch - 13ms/step\n",
      "Epoch 44/300\n",
      "4/4 - 0s - loss: 0.8875 - accuracy: 0.7792 - 50ms/epoch - 13ms/step\n",
      "Epoch 45/300\n",
      "4/4 - 0s - loss: 0.8361 - accuracy: 0.7583 - 52ms/epoch - 13ms/step\n",
      "Epoch 46/300\n",
      "4/4 - 0s - loss: 0.8011 - accuracy: 0.8042 - 53ms/epoch - 13ms/step\n",
      "Epoch 47/300\n",
      "4/4 - 0s - loss: 0.7957 - accuracy: 0.7917 - 50ms/epoch - 12ms/step\n",
      "Epoch 48/300\n",
      "4/4 - 0s - loss: 0.7716 - accuracy: 0.7875 - 56ms/epoch - 14ms/step\n",
      "Epoch 49/300\n",
      "4/4 - 0s - loss: 0.6900 - accuracy: 0.8250 - 53ms/epoch - 13ms/step\n",
      "Epoch 50/300\n",
      "4/4 - 0s - loss: 0.6729 - accuracy: 0.8250 - 51ms/epoch - 13ms/step\n",
      "Epoch 51/300\n",
      "4/4 - 0s - loss: 0.6547 - accuracy: 0.8375 - 51ms/epoch - 13ms/step\n",
      "Epoch 52/300\n",
      "4/4 - 0s - loss: 0.6340 - accuracy: 0.8542 - 50ms/epoch - 13ms/step\n",
      "Epoch 53/300\n",
      "4/4 - 0s - loss: 0.5814 - accuracy: 0.8583 - 52ms/epoch - 13ms/step\n",
      "Epoch 54/300\n",
      "4/4 - 0s - loss: 0.5889 - accuracy: 0.8333 - 52ms/epoch - 13ms/step\n",
      "Epoch 55/300\n",
      "4/4 - 0s - loss: 0.5634 - accuracy: 0.8667 - 49ms/epoch - 12ms/step\n",
      "Epoch 56/300\n",
      "4/4 - 0s - loss: 0.5431 - accuracy: 0.8625 - 51ms/epoch - 13ms/step\n",
      "Epoch 57/300\n",
      "4/4 - 0s - loss: 0.5158 - accuracy: 0.8667 - 51ms/epoch - 13ms/step\n",
      "Epoch 58/300\n",
      "4/4 - 0s - loss: 0.4907 - accuracy: 0.8875 - 50ms/epoch - 13ms/step\n",
      "Epoch 59/300\n",
      "4/4 - 0s - loss: 0.4720 - accuracy: 0.8833 - 50ms/epoch - 13ms/step\n",
      "Epoch 60/300\n",
      "4/4 - 0s - loss: 0.4435 - accuracy: 0.8958 - 53ms/epoch - 13ms/step\n",
      "Epoch 61/300\n",
      "4/4 - 0s - loss: 0.4310 - accuracy: 0.8917 - 51ms/epoch - 13ms/step\n",
      "Epoch 62/300\n",
      "4/4 - 0s - loss: 0.4192 - accuracy: 0.9000 - 49ms/epoch - 12ms/step\n",
      "Epoch 63/300\n",
      "4/4 - 0s - loss: 0.4045 - accuracy: 0.9125 - 51ms/epoch - 13ms/step\n",
      "Epoch 64/300\n",
      "4/4 - 0s - loss: 0.3805 - accuracy: 0.9083 - 51ms/epoch - 13ms/step\n",
      "Epoch 65/300\n",
      "4/4 - 0s - loss: 0.3816 - accuracy: 0.9292 - 50ms/epoch - 13ms/step\n",
      "Epoch 66/300\n",
      "4/4 - 0s - loss: 0.3825 - accuracy: 0.9125 - 50ms/epoch - 12ms/step\n",
      "Epoch 67/300\n",
      "4/4 - 0s - loss: 0.3467 - accuracy: 0.9208 - 48ms/epoch - 12ms/step\n",
      "Epoch 68/300\n",
      "4/4 - 0s - loss: 0.3203 - accuracy: 0.9333 - 48ms/epoch - 12ms/step\n",
      "Epoch 69/300\n",
      "4/4 - 0s - loss: 0.3232 - accuracy: 0.9333 - 51ms/epoch - 13ms/step\n",
      "Epoch 70/300\n",
      "4/4 - 0s - loss: 0.2984 - accuracy: 0.9417 - 52ms/epoch - 13ms/step\n",
      "Epoch 71/300\n",
      "4/4 - 0s - loss: 0.2906 - accuracy: 0.9375 - 51ms/epoch - 13ms/step\n",
      "Epoch 72/300\n",
      "4/4 - 0s - loss: 0.2780 - accuracy: 0.9583 - 51ms/epoch - 13ms/step\n",
      "Epoch 73/300\n",
      "4/4 - 0s - loss: 0.2398 - accuracy: 0.9625 - 50ms/epoch - 12ms/step\n",
      "Epoch 74/300\n",
      "4/4 - 0s - loss: 0.2351 - accuracy: 0.9625 - 49ms/epoch - 12ms/step\n",
      "Epoch 75/300\n",
      "4/4 - 0s - loss: 0.2095 - accuracy: 0.9792 - 50ms/epoch - 12ms/step\n",
      "Epoch 76/300\n",
      "4/4 - 0s - loss: 0.2065 - accuracy: 0.9750 - 50ms/epoch - 12ms/step\n",
      "Epoch 77/300\n",
      "4/4 - 0s - loss: 0.2043 - accuracy: 0.9667 - 54ms/epoch - 13ms/step\n",
      "Epoch 78/300\n",
      "4/4 - 0s - loss: 0.1738 - accuracy: 0.9833 - 52ms/epoch - 13ms/step\n",
      "Epoch 79/300\n",
      "4/4 - 0s - loss: 0.1993 - accuracy: 0.9792 - 51ms/epoch - 13ms/step\n",
      "Epoch 80/300\n",
      "4/4 - 0s - loss: 0.1641 - accuracy: 0.9833 - 50ms/epoch - 13ms/step\n",
      "Epoch 81/300\n",
      "4/4 - 0s - loss: 0.1634 - accuracy: 0.9875 - 51ms/epoch - 13ms/step\n",
      "Epoch 82/300\n",
      "4/4 - 0s - loss: 0.1448 - accuracy: 0.9875 - 50ms/epoch - 12ms/step\n",
      "Epoch 83/300\n",
      "4/4 - 0s - loss: 0.1505 - accuracy: 0.9875 - 52ms/epoch - 13ms/step\n",
      "Epoch 84/300\n",
      "4/4 - 0s - loss: 0.1400 - accuracy: 0.9958 - 50ms/epoch - 13ms/step\n",
      "Epoch 85/300\n",
      "4/4 - 0s - loss: 0.1361 - accuracy: 0.9792 - 50ms/epoch - 12ms/step\n",
      "Epoch 86/300\n",
      "4/4 - 0s - loss: 0.1178 - accuracy: 0.9917 - 50ms/epoch - 13ms/step\n",
      "Epoch 87/300\n",
      "4/4 - 0s - loss: 0.1068 - accuracy: 0.9958 - 53ms/epoch - 13ms/step\n",
      "Epoch 88/300\n",
      "4/4 - 0s - loss: 0.1039 - accuracy: 0.9958 - 50ms/epoch - 12ms/step\n",
      "Epoch 89/300\n",
      "4/4 - 0s - loss: 0.1127 - accuracy: 0.9875 - 50ms/epoch - 13ms/step\n",
      "Epoch 90/300\n",
      "4/4 - 0s - loss: 0.1090 - accuracy: 0.9917 - 49ms/epoch - 12ms/step\n",
      "Epoch 91/300\n",
      "4/4 - 0s - loss: 0.0919 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 92/300\n",
      "4/4 - 0s - loss: 0.0804 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 93/300\n",
      "4/4 - 0s - loss: 0.0760 - accuracy: 0.9917 - 52ms/epoch - 13ms/step\n",
      "Epoch 94/300\n",
      "4/4 - 0s - loss: 0.0730 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 95/300\n",
      "4/4 - 0s - loss: 0.0792 - accuracy: 0.9958 - 51ms/epoch - 13ms/step\n",
      "Epoch 96/300\n",
      "4/4 - 0s - loss: 0.0753 - accuracy: 0.9958 - 51ms/epoch - 13ms/step\n",
      "Epoch 97/300\n",
      "4/4 - 0s - loss: 0.0739 - accuracy: 0.9958 - 52ms/epoch - 13ms/step\n",
      "Epoch 98/300\n",
      "4/4 - 0s - loss: 0.0685 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 99/300\n",
      "4/4 - 0s - loss: 0.0655 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 100/300\n",
      "4/4 - 0s - loss: 0.0542 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 101/300\n",
      "4/4 - 0s - loss: 0.0561 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 102/300\n",
      "4/4 - 0s - loss: 0.0463 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300\n",
      "4/4 - 0s - loss: 0.0520 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 104/300\n",
      "4/4 - 0s - loss: 0.0486 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 105/300\n",
      "4/4 - 0s - loss: 0.0437 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 106/300\n",
      "4/4 - 0s - loss: 0.0432 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 107/300\n",
      "4/4 - 0s - loss: 0.0399 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 108/300\n",
      "4/4 - 0s - loss: 0.0401 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 109/300\n",
      "4/4 - 0s - loss: 0.0396 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 110/300\n",
      "4/4 - 0s - loss: 0.0445 - accuracy: 0.9958 - 49ms/epoch - 12ms/step\n",
      "Epoch 111/300\n",
      "4/4 - 0s - loss: 0.0357 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 112/300\n",
      "4/4 - 0s - loss: 0.0377 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 113/300\n",
      "4/4 - 0s - loss: 0.0340 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 114/300\n",
      "4/4 - 0s - loss: 0.0328 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 115/300\n",
      "4/4 - 0s - loss: 0.0309 - accuracy: 1.0000 - 49ms/epoch - 12ms/step\n",
      "Epoch 116/300\n",
      "4/4 - 0s - loss: 0.0293 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 117/300\n",
      "4/4 - 0s - loss: 0.0315 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 118/300\n",
      "4/4 - 0s - loss: 0.0252 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 119/300\n",
      "4/4 - 0s - loss: 0.0304 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 120/300\n",
      "4/4 - 0s - loss: 0.0279 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 121/300\n",
      "4/4 - 0s - loss: 0.0229 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 122/300\n",
      "4/4 - 0s - loss: 0.0234 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 123/300\n",
      "4/4 - 0s - loss: 0.0229 - accuracy: 1.0000 - 49ms/epoch - 12ms/step\n",
      "Epoch 124/300\n",
      "4/4 - 0s - loss: 0.0229 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 125/300\n",
      "4/4 - 0s - loss: 0.0220 - accuracy: 1.0000 - 49ms/epoch - 12ms/step\n",
      "Epoch 126/300\n",
      "4/4 - 0s - loss: 0.0223 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 127/300\n",
      "4/4 - 0s - loss: 0.0190 - accuracy: 1.0000 - 49ms/epoch - 12ms/step\n",
      "Epoch 128/300\n",
      "4/4 - 0s - loss: 0.0205 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 129/300\n",
      "4/4 - 0s - loss: 0.0210 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 130/300\n",
      "4/4 - 0s - loss: 0.0206 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 131/300\n",
      "4/4 - 0s - loss: 0.0205 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 132/300\n",
      "4/4 - 0s - loss: 0.0192 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 133/300\n",
      "4/4 - 0s - loss: 0.0181 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 134/300\n",
      "4/4 - 0s - loss: 0.0187 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 135/300\n",
      "4/4 - 0s - loss: 0.0165 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 136/300\n",
      "4/4 - 0s - loss: 0.0174 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 137/300\n",
      "4/4 - 0s - loss: 0.0179 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 138/300\n",
      "4/4 - 0s - loss: 0.0157 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 139/300\n",
      "4/4 - 0s - loss: 0.0174 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 140/300\n",
      "4/4 - 0s - loss: 0.0174 - accuracy: 1.0000 - 49ms/epoch - 12ms/step\n",
      "Epoch 141/300\n",
      "4/4 - 0s - loss: 0.0160 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 142/300\n",
      "4/4 - 0s - loss: 0.0146 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 143/300\n",
      "4/4 - 0s - loss: 0.0163 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 144/300\n",
      "4/4 - 0s - loss: 0.0147 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 145/300\n",
      "4/4 - 0s - loss: 0.0146 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 146/300\n",
      "4/4 - 0s - loss: 0.0147 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 147/300\n",
      "4/4 - 0s - loss: 0.0155 - accuracy: 1.0000 - 57ms/epoch - 14ms/step\n",
      "Epoch 148/300\n",
      "4/4 - 0s - loss: 0.0134 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 149/300\n",
      "4/4 - 0s - loss: 0.0140 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 150/300\n",
      "4/4 - 0s - loss: 0.0128 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 151/300\n",
      "4/4 - 0s - loss: 0.0137 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 152/300\n",
      "4/4 - 0s - loss: 0.0142 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 153/300\n",
      "4/4 - 0s - loss: 0.0126 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 154/300\n",
      "4/4 - 0s - loss: 0.0128 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 155/300\n",
      "4/4 - 0s - loss: 0.0119 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 156/300\n",
      "4/4 - 0s - loss: 0.0117 - accuracy: 1.0000 - 56ms/epoch - 14ms/step\n",
      "Epoch 157/300\n",
      "4/4 - 0s - loss: 0.0115 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 158/300\n",
      "4/4 - 0s - loss: 0.0114 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 159/300\n",
      "4/4 - 0s - loss: 0.0114 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 160/300\n",
      "4/4 - 0s - loss: 0.0107 - accuracy: 1.0000 - 57ms/epoch - 14ms/step\n",
      "Epoch 161/300\n",
      "4/4 - 0s - loss: 0.0121 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 162/300\n",
      "4/4 - 0s - loss: 0.0109 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 163/300\n",
      "4/4 - 0s - loss: 0.0102 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 164/300\n",
      "4/4 - 0s - loss: 0.0095 - accuracy: 1.0000 - 48ms/epoch - 12ms/step\n",
      "Epoch 165/300\n",
      "4/4 - 0s - loss: 0.0105 - accuracy: 1.0000 - 49ms/epoch - 12ms/step\n",
      "Epoch 166/300\n",
      "4/4 - 0s - loss: 0.0108 - accuracy: 1.0000 - 50ms/epoch - 12ms/step\n",
      "Epoch 167/300\n",
      "4/4 - 0s - loss: 0.0092 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 168/300\n",
      "4/4 - 0s - loss: 0.0094 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 169/300\n",
      "4/4 - 0s - loss: 0.0099 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 170/300\n",
      "4/4 - 0s - loss: 0.0104 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 171/300\n",
      "4/4 - 0s - loss: 0.0091 - accuracy: 1.0000 - 62ms/epoch - 15ms/step\n",
      "Epoch 172/300\n",
      "4/4 - 0s - loss: 0.0118 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 173/300\n",
      "4/4 - 0s - loss: 0.0105 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 174/300\n",
      "4/4 - 0s - loss: 0.0127 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 175/300\n",
      "4/4 - 0s - loss: 0.0106 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 176/300\n",
      "4/4 - 0s - loss: 0.0098 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 177/300\n",
      "4/4 - 0s - loss: 0.0091 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 178/300\n",
      "4/4 - 0s - loss: 0.0106 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 179/300\n",
      "4/4 - 0s - loss: 0.0095 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 180/300\n",
      "4/4 - 0s - loss: 0.0110 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 181/300\n",
      "4/4 - 0s - loss: 0.0093 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 182/300\n",
      "4/4 - 0s - loss: 0.0098 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 183/300\n",
      "4/4 - 0s - loss: 0.0095 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 184/300\n",
      "4/4 - 0s - loss: 0.0085 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 185/300\n",
      "4/4 - 0s - loss: 0.0084 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 186/300\n",
      "4/4 - 0s - loss: 0.0083 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 187/300\n",
      "4/4 - 0s - loss: 0.0078 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 188/300\n",
      "4/4 - 0s - loss: 0.0079 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 189/300\n",
      "4/4 - 0s - loss: 0.0078 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 190/300\n",
      "4/4 - 0s - loss: 0.0073 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 191/300\n",
      "4/4 - 0s - loss: 0.0078 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 192/300\n",
      "4/4 - 0s - loss: 0.0076 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 193/300\n",
      "4/4 - 0s - loss: 0.0068 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 194/300\n",
      "4/4 - 0s - loss: 0.0076 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 195/300\n",
      "4/4 - 0s - loss: 0.0071 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 196/300\n",
      "4/4 - 0s - loss: 0.0074 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 197/300\n",
      "4/4 - 0s - loss: 0.0072 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 198/300\n",
      "4/4 - 0s - loss: 0.0067 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 199/300\n",
      "4/4 - 0s - loss: 0.0069 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 200/300\n",
      "4/4 - 0s - loss: 0.0067 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 201/300\n",
      "4/4 - 0s - loss: 0.0066 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 202/300\n",
      "4/4 - 0s - loss: 0.0062 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/300\n",
      "4/4 - 0s - loss: 0.0066 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 204/300\n",
      "4/4 - 0s - loss: 0.0067 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 205/300\n",
      "4/4 - 0s - loss: 0.0067 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 206/300\n",
      "4/4 - 0s - loss: 0.0067 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 207/300\n",
      "4/4 - 0s - loss: 0.0063 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 208/300\n",
      "4/4 - 0s - loss: 0.0066 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 209/300\n",
      "4/4 - 0s - loss: 0.0057 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 210/300\n",
      "4/4 - 0s - loss: 0.0063 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 211/300\n",
      "4/4 - 0s - loss: 0.0056 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 212/300\n",
      "4/4 - 0s - loss: 0.0062 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 213/300\n",
      "4/4 - 0s - loss: 0.0075 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 214/300\n",
      "4/4 - 0s - loss: 0.0056 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 215/300\n",
      "4/4 - 0s - loss: 0.0057 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 216/300\n",
      "4/4 - 0s - loss: 0.0054 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 217/300\n",
      "4/4 - 0s - loss: 0.0054 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 218/300\n",
      "4/4 - 0s - loss: 0.0055 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 219/300\n",
      "4/4 - 0s - loss: 0.0053 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 220/300\n",
      "4/4 - 0s - loss: 0.0050 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 221/300\n",
      "4/4 - 0s - loss: 0.0059 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 222/300\n",
      "4/4 - 0s - loss: 0.0046 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 223/300\n",
      "4/4 - 0s - loss: 0.0052 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 224/300\n",
      "4/4 - 0s - loss: 0.0049 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 225/300\n",
      "4/4 - 0s - loss: 0.0054 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 226/300\n",
      "4/4 - 0s - loss: 0.0056 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 227/300\n",
      "4/4 - 0s - loss: 0.0051 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 228/300\n",
      "4/4 - 0s - loss: 0.0052 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 229/300\n",
      "4/4 - 0s - loss: 0.0053 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 230/300\n",
      "4/4 - 0s - loss: 0.0052 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 231/300\n",
      "4/4 - 0s - loss: 0.0051 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 232/300\n",
      "4/4 - 0s - loss: 0.0051 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 233/300\n",
      "4/4 - 0s - loss: 0.0054 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 234/300\n",
      "4/4 - 0s - loss: 0.0046 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 235/300\n",
      "4/4 - 0s - loss: 0.0049 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 236/300\n",
      "4/4 - 0s - loss: 0.0048 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 237/300\n",
      "4/4 - 0s - loss: 0.0047 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 238/300\n",
      "4/4 - 0s - loss: 0.0051 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 239/300\n",
      "4/4 - 0s - loss: 0.0040 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 240/300\n",
      "4/4 - 0s - loss: 0.0049 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 241/300\n",
      "4/4 - 0s - loss: 0.0042 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 242/300\n",
      "4/4 - 0s - loss: 0.0043 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 243/300\n",
      "4/4 - 0s - loss: 0.0044 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 244/300\n",
      "4/4 - 0s - loss: 0.0045 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 245/300\n",
      "4/4 - 0s - loss: 0.0042 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 246/300\n",
      "4/4 - 0s - loss: 0.0040 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 247/300\n",
      "4/4 - 0s - loss: 0.0039 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 248/300\n",
      "4/4 - 0s - loss: 0.0042 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 249/300\n",
      "4/4 - 0s - loss: 0.0046 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 250/300\n",
      "4/4 - 0s - loss: 0.0042 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 251/300\n",
      "4/4 - 0s - loss: 0.0038 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 252/300\n",
      "4/4 - 0s - loss: 0.0038 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 253/300\n",
      "4/4 - 0s - loss: 0.0039 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 254/300\n",
      "4/4 - 0s - loss: 0.0041 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 255/300\n",
      "4/4 - 0s - loss: 0.0036 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 256/300\n",
      "4/4 - 0s - loss: 0.0034 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 257/300\n",
      "4/4 - 0s - loss: 0.0036 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 258/300\n",
      "4/4 - 0s - loss: 0.0033 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 259/300\n",
      "4/4 - 0s - loss: 0.0033 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 260/300\n",
      "4/4 - 0s - loss: 0.0036 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 261/300\n",
      "4/4 - 0s - loss: 0.0037 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 262/300\n",
      "4/4 - 0s - loss: 0.0035 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 263/300\n",
      "4/4 - 0s - loss: 0.0038 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 264/300\n",
      "4/4 - 0s - loss: 0.0043 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 265/300\n",
      "4/4 - 0s - loss: 0.0032 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 266/300\n",
      "4/4 - 0s - loss: 0.0038 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 267/300\n",
      "4/4 - 0s - loss: 0.0036 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 268/300\n",
      "4/4 - 0s - loss: 0.0038 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 269/300\n",
      "4/4 - 0s - loss: 0.0034 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 270/300\n",
      "4/4 - 0s - loss: 0.0033 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 271/300\n",
      "4/4 - 0s - loss: 0.0030 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 272/300\n",
      "4/4 - 0s - loss: 0.0034 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 273/300\n",
      "4/4 - 0s - loss: 0.0034 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 274/300\n",
      "4/4 - 0s - loss: 0.0035 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 275/300\n",
      "4/4 - 0s - loss: 0.0030 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 276/300\n",
      "4/4 - 0s - loss: 0.0036 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 277/300\n",
      "4/4 - 0s - loss: 0.0033 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 278/300\n",
      "4/4 - 0s - loss: 0.0032 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 279/300\n",
      "4/4 - 0s - loss: 0.0032 - accuracy: 1.0000 - 56ms/epoch - 14ms/step\n",
      "Epoch 280/300\n",
      "4/4 - 0s - loss: 0.0030 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 281/300\n",
      "4/4 - 0s - loss: 0.0030 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 282/300\n",
      "4/4 - 0s - loss: 0.0031 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 283/300\n",
      "4/4 - 0s - loss: 0.0031 - accuracy: 1.0000 - 55ms/epoch - 14ms/step\n",
      "Epoch 284/300\n",
      "4/4 - 0s - loss: 0.0027 - accuracy: 1.0000 - 56ms/epoch - 14ms/step\n",
      "Epoch 285/300\n",
      "4/4 - 0s - loss: 0.0032 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 286/300\n",
      "4/4 - 0s - loss: 0.0029 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 287/300\n",
      "4/4 - 0s - loss: 0.0032 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 288/300\n",
      "4/4 - 0s - loss: 0.0033 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 289/300\n",
      "4/4 - 0s - loss: 0.0031 - accuracy: 1.0000 - 50ms/epoch - 13ms/step\n",
      "Epoch 290/300\n",
      "4/4 - 0s - loss: 0.0032 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 291/300\n",
      "4/4 - 0s - loss: 0.0030 - accuracy: 1.0000 - 51ms/epoch - 13ms/step\n",
      "Epoch 292/300\n",
      "4/4 - 0s - loss: 0.0028 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 293/300\n",
      "4/4 - 0s - loss: 0.0029 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 294/300\n",
      "4/4 - 0s - loss: 0.0029 - accuracy: 1.0000 - 54ms/epoch - 14ms/step\n",
      "Epoch 295/300\n",
      "4/4 - 0s - loss: 0.0029 - accuracy: 1.0000 - 54ms/epoch - 13ms/step\n",
      "Epoch 296/300\n",
      "4/4 - 0s - loss: 0.0027 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 297/300\n",
      "4/4 - 0s - loss: 0.0029 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 298/300\n",
      "4/4 - 0s - loss: 0.0027 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n",
      "Epoch 299/300\n",
      "4/4 - 0s - loss: 0.0028 - accuracy: 1.0000 - 52ms/epoch - 13ms/step\n",
      "Epoch 300/300\n",
      "4/4 - 0s - loss: 0.0029 - accuracy: 1.0000 - 53ms/epoch - 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf0011b4f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer() # Instantiating a new transformer model\n",
    "src_seqs, tgt_seqs, tgt_labels = zip(*dataset)\n",
    "train = [tf.cast(src_seqs, dtype=tf.float32), tf.cast(tgt_seqs, dtype=tf.float32)] # Cast the tuples to tensors\n",
    "\n",
    "transformer.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "transformer.fit(train, tf.cast(tgt_labels, dtype=tf.float32), verbose=2, batch_size=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcZ9Erkn9Z9h"
   },
   "source": [
    "# Translation testing\n",
    "Since we are using only 20 sentences for training demonstration, this model is not expected to work well in arbitrary testing examples. In order to make sure that the model works, we will translate a training source sentence and compare the prediction and the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tb6DEeIuLSOt"
   },
   "outputs": [],
   "source": [
    "def translate(model, source_sentence, target_sentence_start=[['<start>']]):\n",
    "    if np.ndim(source_sentence) == 1: # Create a batch of 1 the input is a sentence\n",
    "        source_sentence = [source_sentence]\n",
    "    \n",
    "    if np.ndim(target_sentence_start) == 1:\n",
    "        target_sentence_start = [target_sentence_start]\n",
    "  \n",
    "    # Tokenizing and padding\n",
    "    source_seq = source_tokenizer.texts_to_sequences(source_sentence)\n",
    "    source_seq = tf.keras.preprocessing.sequence.pad_sequences(source_seq, padding='post', maxlen=15)\n",
    "    predict_seq = target_tokenizer.texts_to_sequences(target_sentence_start)\n",
    "\n",
    "    predict_sentence = list(target_sentence_start[0]) # Deep copy here to prevent updates on target_sentence_start\n",
    "    \n",
    "    while predict_sentence[-1] != '<end>' and len(predict_seq) < max_token_length:\n",
    "        predict_output = model([np.array(source_seq), np.array(predict_seq)], training=None)\n",
    "        predict_label = tf.argmax(predict_output, axis=-1) # Pick the label with highest softmax score\n",
    "        predict_seq = tf.concat([predict_seq, predict_label], axis=-1) # Updating the prediction sequence\n",
    "        predict_sentence.append(target_tokenizer.index_word[predict_label[0][0].numpy()])\n",
    "\n",
    "    return predict_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "TUJVgmDhRQLm",
    "outputId": "6d4dd7e9-d432-4726-bca2-138dfcfd2953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:  <start> Another coffee , please . <end>\n",
      "Target sentence:  <start> Otro cafe , por favor . <end>\n",
      "Predicted sentence:  <start> otro cafe , por favor . <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Source sentence: \", source_sentences[10])\n",
    "print(\"Target sentence: \", target_sentences[10])\n",
    "print(\"Predicted sentence: \", ' '.join(translate(transformer, source_sentences[10].split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it! Everything is working as expected and you have build your first own working transformer. Now you can play around with it. Just take a bigger dataset for example and retrain everything. Change the model parameters and try out which size of transformer models will fit into your GPU memory. Which impact does the bigger model has on the training time and your models accuracy? Try, to build a transformer for another task, like sentiment analysis for example.\n",
    "\n",
    "# Further recommendations\n",
    "If you are interested in some bigger pretrained models checkout [Huggingface](https://huggingface.co/docs/transformers/index) and learn how to finetune them to your own needs or just signup for the [OpenAI GPT-3 Playground](https://beta.openai.com/playground) to play around with a realy big transformer and learn what a huge amount of different tasks a single pre-trained transformer can solve."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Easy Transformer.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
