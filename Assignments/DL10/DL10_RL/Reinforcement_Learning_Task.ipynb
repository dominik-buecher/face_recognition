{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Introduction\n",
    "## Tabular Q-Learning\n",
    "(by: [Nicolaj Stache](mailto:Nicolaj.Stache@hs-heilbronn.de), and [Pascal Graf](mailto:pascal.graf@hs-heilbronn.de), both: Heilbronn University, Germany, June 2022) \n",
    "\n",
    "In this notebook we look at a simple reinforcement learning problem and the tabular Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Table of Contents:\n",
    "### 1. [Imports](#imports)\n",
    "\n",
    "### 2. [Environment](#environment)\n",
    "\n",
    "### 3. [Training](#training)\n",
    "\n",
    "### 4. [Evaluation](#evaluation)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment <a class=\"anchor\" id=\"environment\"></a>\n",
    "The environment we're going to solve is called \"Grid World\". The respective Python file is stored in a folder along with this notebook. The task is to reach the terminal state that yields a positive reward in a grid with arbitrary size. Every other step in the environment leads to a slight negative reward. There are also obstacles randomly distributed over the environment and a terminal field to avoid which yields a negative reward.\n",
    "<hr>\n",
    "\n",
    "**TO DO:** Take a look into the grid_world file and try to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.grid_world import Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new random Grid World and explore the environment\n",
    "Initialize the grid environment, then print its structure as well as the the rewards that are obtained reaching each state.\n",
    "<hr>\n",
    "\n",
    "**TO DO:** Make sure the target field is reachable. Each function call will create a new random grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Grid World\n",
    "grid = Grid(5, 4, step_reward=-0.1, obstacles=3)\n",
    "# Print grid structure\n",
    "print(\"Grid World:\")\n",
    "grid.print_grid()\n",
    "\n",
    "print(\"Rewards:\")\n",
    "grid.print_grid_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training <a class=\"anchor\" id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "The success of the reinforcement learning algorithm heavily depends of the choice of parameters. Among these are the learning rate $\\alpha$, the discount factor $\\gamma$ and the exploration parameters. \n",
    "<hr>\n",
    "\n",
    "**TO DO:** After successfully training the algorithm with the default parameters, try modifying `EPSILON`, `EPSILON_DECAY`, `GAMMA` and `ALPHA` to get a feel for their effects on the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training episodes\n",
    "EPISODES = 10000\n",
    "# Discount Factor\n",
    "GAMMA = 0.9\n",
    "# Learning Rate\n",
    "ALPHA = 0.05\n",
    "# Epsilon\n",
    "EPSILON = 1\n",
    "# Decay of random actions\n",
    "EPSILON_DECAY = 0.95\n",
    "# Action space (Up, Down, Left, Right)\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm\n",
    "Q-Learning is an online, off-policy, non-model-based, bootstrapping reinforcement learning algorithm. We will use a modified implementation taken from [here](https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python) in attempt to solve the Grid World environment. Underneath you can see the pseudo code for tabular Q-Learning.\n",
    "\n",
    "\n",
    "#### Q-Learning Pseudo Code\n",
    "<hr>\n",
    "\n",
    "- Initialize:\n",
    "    - Step Size $\\alpha \\in (0,1]$ \n",
    "    - $Q(s,a)$ dictionary for all $s\\in S, a \\in A$.\n",
    "    - $\\epsilon > 0$\n",
    "- Loop for each episode:\n",
    "    1. Choose $A$ from $S$ using an $\\epsilon$-greedy policy.\n",
    "    2. Take action $A$, observe $R, S'$.\n",
    "    3. $Q(S,A)\\leftarrow Q(S,A) + \\alpha [R+\\gamma max_a Q(S', a)-Q(S,A)]$\n",
    "    4. $S\\leftarrow S'$\n",
    "    \n",
    "    until $S$ is terminal\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous functions\n",
    "\n",
    "In order to perform and evaluate the training we need some helper functions. \n",
    "\n",
    "The **`max_dict`** function accepts a dictionary as parameter, loops through the dictionary and returns the maximum value as well as its key.\n",
    "\n",
    "The **`act_epsilon_greedy`** function selects an action from a dictionary according to the epsilon-greedy algorithm. This means, in $\\epsilon$ percent of the cases it returns a random action, otherwise the action is chosen greedily according to a action-value dictionary.\n",
    "<hr>\n",
    "\n",
    "\n",
    "**TO DO:** \n",
    "1. Complete the **`max_dict`** function to return the maximum value and its respective key from a dictionary.\n",
    "\n",
    "2. Complete the **`act_epsilon_greedy`** function to select a random action from `ALL_POSSIBLE_ACTIONS` if a random value is smaller or equal to $\\epsilon$. Otherwise act greedily with respect to $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    '''Returns the argmax (key) and max (value) from a dictionary.'''\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    # TO DO\n",
    "    return max_key, max_val\n",
    "\n",
    "def act_epsilon_greedy(Qs, eps=0.1):\n",
    "    '''Selects an epsilon greedy action given the action-values for a state and epsilon.'''\n",
    "    # TO DO\n",
    "    a, _ = max_dict(Qs)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "<hr>\n",
    "\n",
    "**TO DO:** Complete the following code to correspond to the pseude code above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial exploration\n",
    "eps = EPSILON\n",
    "\n",
    "# Initialize an empty dictionary for the action-value function Q(s,a)\n",
    "Q = {}\n",
    "# Initialize an empty dictionary for the number of times action a has been chosen in state s.\n",
    "action_visits = {}\n",
    "# Initialize an empty dictionary for the number of times state s has been visited.\n",
    "state_visits = {}\n",
    "\n",
    "# For each state corresponding to each field in the environment, create a new dictionary inside the dictionaries.\n",
    "for row in range(grid.height):\n",
    "    for col in range(grid.width):\n",
    "        Q[(row, col)] = {}\n",
    "        action_visits[(row, col)] = {}\n",
    "        state_visits[(row, col)] = 1\n",
    "        # For each action in each state create a new entry in the dictonary setting its estimated value to 0 and its visits to 1.\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[(row, col)][a] = 0\n",
    "            action_visits[(row, col)][a] = 1\n",
    "            \n",
    "# Keep track of the maximum value changes during each episode so we can plot a learning curve after the training       \n",
    "deltas = []\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "# Play EPISODES number of episodes in the environment while learning after each step.\n",
    "for e in range(EPISODES):\n",
    "    # Every 1000 steps print the current episode number and epsilon\n",
    "    if e % 1000 == 0:\n",
    "        print(\"Episode: {}, Epsilon: {}\".format(e, eps))\n",
    "        \n",
    "    # Decrease epsilon every 100 episodes to approach a more deterministic policy.\n",
    "    if e % 100 == 0 and eps>0:\n",
    "        eps *= EPSILON_DECAY\n",
    "\n",
    "    # Set the starting state\n",
    "    grid.reset()\n",
    "    s = grid.get_current_state()\n",
    "\n",
    "    biggest_change = 0\n",
    "    while not grid.is_state_terminal():\n",
    "        # Choose an action epsilon-greedily.\n",
    "        a = act_epsilon_greedy(Q[s], eps=eps)\n",
    "        r = grid.move(a)\n",
    "        next_s = grid.get_current_state()\n",
    "        \n",
    "        # Update the state and action visits number.\n",
    "        state_visits[s] += 1\n",
    "        action_visits[s][a] += 1\n",
    "        \n",
    "        # TO DO: Implement the Q-Learning update formula from the pseudo code.\n",
    "        # Update Q(s,a)\n",
    "\n",
    "        # Next state becomes current state.\n",
    "        s = next_s\n",
    "\n",
    "    deltas.append(biggest_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "Now we want to evaluate the learning performance of our Q-learning agent.\n",
    "First, we take a look at the plot of the biggest value changes for each episodes. These values should approach zero over time as our estimate of Q converges towards the real values.\n",
    "\n",
    "After that we use our action-value dictionary to determine the value of each state and the policy which is the greedy action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the biggest value change in each episode.\n",
    "plt.plot(deltas)\n",
    "plt.show()\n",
    "\n",
    "# Determine the policy from Q and find the value of each state which corresponds to the maximum Q-value.\n",
    "policy = {}\n",
    "V = {}\n",
    "for row in range(grid.height):\n",
    "    for col in range(grid.width):\n",
    "        # Determine the greedy action in each state.\n",
    "        a, max_q = max_dict(Q[(row, col)])\n",
    "        policy[(row, col)] = a\n",
    "        # Get the value V for each state.\n",
    "        V[(row, col)] = max_q\n",
    "\n",
    "print(\"State Values:\")\n",
    "grid.print_state_values(V)\n",
    "print(\"Policy:\")\n",
    "grid.print_policy(policy)\n",
    "print(\"State Visits:\")\n",
    "grid.print_state_visits(state_visits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Upper Confidence Bound (UCB)\n",
    "<hr>\n",
    "\n",
    "**TO DO (Task 2):** Complete the `act_ucb` function to act according to the upper confidence bound. Then copy and adapt the training loop from above. Evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_ucb(Qs, t, Ns, c=0.1):\n",
    "    '''Selects an UpperConfidenceBound action given the action-values for a state, \n",
    "    the number of times a state has been visited and the number of times every action has been taken from\n",
    "    the current state.'''\n",
    "    # TO DO\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training episodes\n",
    "EPISODES = 10000\n",
    "# Discount Factor\n",
    "GAMMA = 0.9\n",
    "# Learning Rate\n",
    "ALPHA = 0.05\n",
    "# UCB Weight\n",
    "C = 50.0\n",
    "# Action space (Up, Down, Left, Right)\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
