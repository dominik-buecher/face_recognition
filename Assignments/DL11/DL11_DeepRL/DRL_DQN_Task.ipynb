{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "## Deep Q-Learning / Deep Q-Network (DQN)\n",
    "(by: [Nicolaj Stache](mailto:Nicolaj.Stache@hs-heilbronn.de), and [Pascal Graf](mailto:pascal.graf@hs-heilbronn.de), both: Heilbronn University, Germany, June 2022) \n",
    "\n",
    "In this notebook we solve reinforcement learning problems utilizing DQN with networks created in Tensorflow.\n",
    "\n",
    "The learning environments shown in this notebook have been created by **[OpenAI](https://openai.com/)**. OpenAI provides a library of diffrent reinforcement learning environments called **[Gym](https://www.gymlibrary.ml/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Table of Contents:\n",
    "### 1. [Imports](#imports)\n",
    "\n",
    "### 2. [Environment](#environment)\n",
    "\n",
    "### 3. [Training](#training)\n",
    "\n",
    "### 4. [Evaluation](#evaluation)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAI Gym\n",
    "In order for this notebook to run you need to open a new terminal, activate your conda environment and install OpenAI Gym by typing `pip install -U gym`. You might also need to install PyGame which is used to render the environments by typing `pip install pygame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple, deque\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Subtract, Add\n",
    "# from tensorflow_addons.layers import NoisyDense # --> first pip install tensorflow_addons\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment <a class=\"anchor\" id=\"environment\"></a>\n",
    "The environment we're going to solve is called \"Cart Pole v1\". \n",
    "\n",
    "<hr>\n",
    "\n",
    "**CartPole-v1**\n",
    "\n",
    "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\n",
    "<hr>\n",
    "\n",
    "\n",
    "First however, let's see how many environments are available to us. After finishing the Cart Pole environment feel free to try any other environment (*Acrobot-v1* or *LunarLander-v2* are recommended). Some environments might require additional packages like *mujoco* or *box2d* which can be installed using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "LunarLander-v2\n",
      "LunarLanderContinuous-v2\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v1\n",
      "CarRacingDomainRandomize-v1\n",
      "CarRacingDiscrete-v1\n",
      "CarRacingDomainRandomizeDiscrete-v1\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n"
     ]
    }
   ],
   "source": [
    "for key, val in gym.envs.registry.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Environment and start interacting\n",
    "\n",
    "Now take a look at todays environment along with its so called action and state/observation space. The meaning of each value in the observation space can be looked up in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------\n",
      "Action Space: Discrete(2)\n",
      "Observation Space: (4,)\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "Sample Action: 1\n",
      "Sample Observation: \n",
      "     Cart Position: 0.03574042767286301 \n",
      "     Cart Velocity: -0.04664144292473793 \n",
      "     Pole Angle: 0.02296554483473301 \n",
      "     Pole Angular Velocity: -0.03243443742394447 \n",
      "----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenAI Gym Environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "# Get action space and observation space sizes\n",
    "print('''\n",
    "----------------------------------------------\n",
    "Action Space: {}\n",
    "Observation Space: {}\n",
    "----------------------------------------------\n",
    "'''.format(env.action_space, env.observation_space.shape))\n",
    "\n",
    "# Get a sample observation and action\n",
    "print('''\n",
    "----------------------------------------------\n",
    "Sample Action: {}\n",
    "Sample Observation: \n",
    "     Cart Position: {} \n",
    "     Cart Velocity: {} \n",
    "     Pole Angle: {} \n",
    "     Pole Angular Velocity: {} \n",
    "----------------------------------------------\n",
    "'''.format(env.action_space.sample(), *env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can also be rendered. Here, random actions are executed which will lead to a fast termination of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 11 timesteps with a reward of 11.0\n",
      "Episode finished after 19 timesteps with a reward of 19.0\n",
      "Episode finished after 21 timesteps with a reward of 21.0\n",
      "Episode finished after 38 timesteps with a reward of 38.0\n",
      "Episode finished after 36 timesteps with a reward of 36.0\n",
      "Episode finished after 13 timesteps with a reward of 13.0\n",
      "Episode finished after 16 timesteps with a reward of 16.0\n",
      "Episode finished after 20 timesteps with a reward of 20.0\n",
      "Episode finished after 9 timesteps with a reward of 9.0\n",
      "Episode finished after 30 timesteps with a reward of 30.0\n",
      "Episode finished after 25 timesteps with a reward of 25.0\n",
      "Episode finished after 15 timesteps with a reward of 15.0\n",
      "Episode finished after 12 timesteps with a reward of 12.0\n",
      "Episode finished after 14 timesteps with a reward of 14.0\n",
      "Episode finished after 36 timesteps with a reward of 36.0\n",
      "Episode finished after 23 timesteps with a reward of 23.0\n",
      "Episode finished after 15 timesteps with a reward of 15.0\n",
      "Episode finished after 19 timesteps with a reward of 19.0\n",
      "Episode finished after 53 timesteps with a reward of 53.0\n",
      "Episode finished after 25 timesteps with a reward of 25.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# Play some test episodes\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps with a reward of {}\".format(t+1, episode_reward))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training <a class=\"anchor\" id=\"training\"></a>\n",
    "\n",
    "In this part of the notebook a simple Deep Q-Learning algorithm is implemented and trained on the previously explored Gym environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q - Learning / Deep Q - Networks (DQN) Pseudo Code\n",
    "<hr>\n",
    "\n",
    "1. Initialize:\n",
    "    - Network $Q(s,a)$ and target network $\\hat{Q}(s,a)$ with random weights.\n",
    "    - $\\epsilon \\leftarrow 1.0$\n",
    "    - Empty Replay Buffer\n",
    "2. With probability $\\epsilon$, select a random action $a$, otherwise, $a=argmax_a Q(s,a)$.\n",
    "3. Execute action $a$ and observe the reward $r$ and the next state $s'$.\n",
    "4. Store transition $(s,a,r,s')$ in the replay buffer.\n",
    "5. Sample a random mini-batch of transitions from the replay buffer.\n",
    "6. For every transition in the batch calculate the target $t=r$ if the episode has ended at this step, or $t=r+\\gamma* max_{a'}\\hat{Q}(s',a')$ otherwise.\n",
    "7. Train the network $Q$ utilizing SGD with the mean squared error.\n",
    "8. Every $N$ steps, copy weights from $Q$ to $\\hat{Q}$.\n",
    "9. Repeat from step 2 until converged.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Components\n",
    "\n",
    "In order to implement DQN we need multiple components which shall be implemented independently in an object oriented way.\n",
    "\n",
    "- **DQN Agent:** Class containing the neural networks, chooses actions given a state, manages the learning process given a batch from the replay buffer.\n",
    "- **Replay Buffer:** Stores transitions $(s,a,r,s')$ and yields batches for training.\n",
    "- **Tensorboard Logger:** Tracks the training process by logging different parameters at each training step.\n",
    "- **Training Loop:** Just a function perfoming the interaction between the previously listed classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > DQN Agent\n",
    "\n",
    "**TODO:** \n",
    "- Build a dense network.\n",
    "- Compile the built model.\n",
    "- Implement the DQN update function.\n",
    "- Implement an Epsilon-Greedy Policy.\n",
    "- **After successful training:** \n",
    "Implement some of the discussed DQN improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent Class\n",
    "    A Deep Q-Learning Agent capable of acting in a discrete action space Gym environment utilizing\n",
    "    an epsilon-greedy policy and a dense neural network.            \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 observation_shape: tuple,\n",
    "                 action_shape: int,\n",
    "                 sync_steps: int = 1000,\n",
    "                 gamma: float = 0.95,\n",
    "                 epsilon: float = 1.0,\n",
    "                 epsilon_decay: float = 0.995,\n",
    "                 epsilon_min: float = 0.01,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 double: bool = False,\n",
    "                 dueling: bool = False,\n",
    "                 noisy: bool = False,\n",
    "                 units: int = 32,\n",
    "                 batch_size: int = 32\n",
    "                 ):\n",
    "        # Store the given parameters as class variables\n",
    "        self.observation_shape = observation_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sync_steps = sync_steps\n",
    "        self.n_step = 0\n",
    "        self.double = double\n",
    "        self.dueling = dueling\n",
    "        self.noisy = noisy\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Build the network model\n",
    "        self.model= self._build_model(observation_shape, action_shape)\n",
    "        \n",
    "        # Create a target model by cloning the existing network\n",
    "        self.target_model = keras.models.clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # TODO: Compile the model using the Adam optimizer and the MSE loss with the given learning rate.\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "    def _build_model(self,\n",
    "                     observation_shape: tuple,\n",
    "                     action_shape: tuple) -> keras.Model:\n",
    "        # TODO: \n",
    "            # Build a Dense Neural Network with two hidden layers.\n",
    "            # Each layer should have \"self.units\" neurons and \"relu\" activation function.\n",
    "            # The input size corresponds to the observation shape.\n",
    "            # The output size corresponds to the action shape.\n",
    "        # TODO: \n",
    "            # After you've successfully trained the vanilla DQN algorithm (or your waiting time is too long)\n",
    "            # try to replace the normal Dense layer in your Network with Noisy Dense layer. This will replace the \n",
    "            # epsilon-greedy action-selection method.\n",
    "        return model\n",
    "\n",
    "    def act(self,\n",
    "            state: tuple):\n",
    "        ## Epsilon-Greedy Policy ##\n",
    "        \n",
    "        # Random action\n",
    "        if np.random.random() <= self.epsilon and not self.noisy:\n",
    "            # TODO: return a random integer between 0 and \"self.action_shape\".\n",
    "        \n",
    "        # Greedy action\n",
    "        # TODO: Given the current state, return the greedy action.\n",
    "        return action[0]\n",
    "\n",
    "    def learn(self,\n",
    "              replay_batch: list) -> float:        \n",
    "        state_batch = replay_batch[\"state_batch\"]\n",
    "        action_batch = replay_batch[\"action_batch\"]\n",
    "        reward_batch = replay_batch[\"reward_batch\"]\n",
    "        next_state_batch = replay_batch[\"next_state_batch\"]\n",
    "        done_batch = replay_batch[\"done_batch\"]\n",
    "        \n",
    "        # TODO: Set the target to the immediate reward\n",
    "        target_batch = reward_batch\n",
    "        # TODO:\n",
    "            # If the state is not terminal, calculate the Q-Value target utilizing the next state and the current reward.\n",
    "            # t = 𝑟 + 𝛾 * 𝑚𝑎𝑥_𝑎′ 𝑄̂(𝑠′,𝑎′) (see pseudo code above)\n",
    "        # TODO:\n",
    "            # After you've successfully trained the vanilla DQN algorithm (or your waiting time is too long)\n",
    "            # try to deploy some of the improvement techniques that have been proposed.\n",
    "            # Search for \"Double DQN\" and try to implement the simple changes in training process.\n",
    "            \n",
    "        # Set the Q value of the chosen action to the target.\n",
    "        q_batch = self.model(state_batch).numpy()\n",
    "        q_batch[np.arange(self.batch_size), action_batch.astype(int)] = target_batch\n",
    "\n",
    "        # Train the network on the training batch.\n",
    "        value_loss = self.model.train_on_batch(state_batch, q_batch)\n",
    "        \n",
    "        # Decrease Epsilon to approach a more deterministic policy.\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        # Every n steps synchronise the target model with the q-model.\n",
    "        self.n_step += 1\n",
    "        if self.n_step >= self.sync_steps:\n",
    "            self.sync_models()\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def sync_models(self):\n",
    "        # Synchronise the target model with the q-model.\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.n_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Replay Buffer\n",
    "\n",
    "The replay buffer is filled with experiences from the agent's interaction with the environment. On demand it returns a random batch of samples from the buffer.\n",
    "\n",
    "**TODO:** Read and try to understand the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,\n",
    "                 capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.collected_samples = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "      \n",
    "    def append(self, s, a, r, next_s, done):\n",
    "        self.buffer.append({\"state\": s, \"action\":a, \"reward\":r, \"next_state\":next_s, \"done\":done})\n",
    "        self.collected_samples += 1\n",
    "\n",
    "    def sample(self,\n",
    "               batch_size: int) -> dict:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        replay_batch = [self.buffer[idx] for idx in indices]\n",
    "        replay_batch = self.batch_to_arrays(replay_batch)\n",
    "        self.collected_samples = 0\n",
    "        return replay_batch\n",
    "    \n",
    "    def batch_to_arrays(self, replay_batch: list) -> dict:\n",
    "        state_batch = np.zeros((len(replay_batch), *replay_batch[0][\"state\"].shape))\n",
    "        action_batch = np.zeros(len(replay_batch))\n",
    "        reward_batch = np.zeros(len(replay_batch))\n",
    "        next_state_batch = np.zeros((len(replay_batch), *replay_batch[0][\"state\"].shape))\n",
    "        done_batch = np.zeros(len(replay_batch), dtype=bool)\n",
    "        \n",
    "        for idx, sample in enumerate(replay_batch):\n",
    "            state_batch[idx] = sample[\"state\"]\n",
    "            action_batch[idx] = sample[\"action\"]\n",
    "            reward_batch[idx] = sample[\"reward\"]\n",
    "            next_state_batch[idx] = sample[\"next_state\"]\n",
    "            done_batch[idx] = sample[\"done\"]\n",
    "            \n",
    "        return {\"state_batch\": state_batch, \"action_batch\":action_batch, \n",
    "                \"reward_batch\": reward_batch, \"next_state_batch\": next_state_batch, \n",
    "                \"done_batch\": done_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Tensorboard Logger\n",
    "The tensorboard logger utilizes the tensorflow feature to log scalar values to an event file. This event file can be plotted by starting a tensorboard server in the local webbrowser by typing `tensorboard --logdir summaries` to a console.\n",
    "\n",
    "**TODO:** Read and try to understand the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, log_dir=\"./summaries\"):\n",
    "        self.log_dir = log_dir\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        self.running_avg_dict = {}\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(tag, value, step)\n",
    "\n",
    "    def log_running_average(self, tag, value, run_avg_len=20):\n",
    "        if tag not in self.running_avg_dict:\n",
    "            self.running_avg_dict[tag] = deque(maxlen=run_avg_len)\n",
    "        self.running_avg_dict[tag].append(value)\n",
    "\n",
    "    def get_running_average(self, tag):\n",
    "        return np.mean(self.running_avg_dict[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "**TODO:** Try different training parameters and see how they influence the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Episodes Played\n",
    "EPISODES = 5000\n",
    "# Memorybatch Size\n",
    "BATCH_SIZE = 32\n",
    "# Discount Factor\n",
    "GAMMA = 0.99\n",
    "# Neurons per Dense layer\n",
    "UNITS = 32\n",
    "# Epsilon Greedy\n",
    "EPSILON_DECAY = 0.999\n",
    "# Model Synchronization Steps\n",
    "SYNC_STEP = 500\n",
    "# Adam learning Rate\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Replay Buffer capacity and minimum size before training\n",
    "BUFFER_CAPACITY = 100000\n",
    "TRAINING_START_SIZE = 10000\n",
    "\n",
    "DUELING = False # Don't change this parameter at first.\n",
    "DOUBLE = False # Don't change this parameter at first.\n",
    "NOISY = False # Don't change this parameter at first.\n",
    "MAX_STEPS = 500 # Don't change this parameter at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Training Loop\n",
    "\n",
    "**TODO:** To monitor the training progress open a command line in your working directory and enter `tensorboard --logdir summaries`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI Gym Environment.\n",
    "env = gym.make('CartPole-v1')\n",
    "  \n",
    "# Create a DQN Agent instance.\n",
    "dqn_agent = DQNAgent(observation_shape=env.observation_space.shape, \n",
    "                     action_shape=env.action_space.n,\n",
    "                     sync_steps=SYNC_STEP,\n",
    "                     units=UNITS,\n",
    "                     epsilon_decay=EPSILON_DECAY,\n",
    "                     learning_rate=LEARNING_RATE,\n",
    "                     double=DOUBLE,\n",
    "                     dueling=DUELING,\n",
    "                     noisy=NOISY,\n",
    "                     batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create a Tensorboard Logger instance.\n",
    "logging_name = datetime.strftime(datetime.now(), '%y%m%d_%H%M%S_DQN')\n",
    "logger = Logger(os.path.join(\"summaries\", logging_name))\n",
    "\n",
    "# Initialize a replay buffer instance.\n",
    "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "\n",
    "\n",
    "# Run episodes in the environment\n",
    "training_step = 0\n",
    "# Save the best reward (averaged over 20 episodes)\n",
    "best_avg_reward = -1000\n",
    "\n",
    "t = trange(EPISODES, desc='', leave=True)   \n",
    "for e in t:\n",
    "    # Reset the environment and obtain the first observation\n",
    "    state = env.reset()\n",
    "\n",
    "    # Keep track of the summed reward\n",
    "    episode_reward = 0\n",
    "\n",
    "    for time_step in range(MAX_STEPS):\n",
    "        # Choose an action.\n",
    "        action = dqn_agent.act(state)\n",
    "        # Perform the action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += np.sum(reward)\n",
    "\n",
    "        # Append the current environment info to the replay buffer.\n",
    "        replay_buffer.append(state, action, reward, next_state, done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "\n",
    "        # If enough samples have been collected acquire memories\n",
    "        # from the replay buffer and use them for training.\n",
    "        if len(replay_buffer) >= TRAINING_START_SIZE and time_step % 4 == 0:\n",
    "                samples = replay_buffer.sample(BATCH_SIZE)\n",
    "                value_loss = dqn_agent.learn(samples)\n",
    "                logger.log_scalar(\"Misc/ValueLoss\", value_loss, training_step)\n",
    "                training_step += 1\n",
    "        # If the episode is over break the loop and start with the next episode.\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Print relevant information.\n",
    "    t.set_description(\"episode: {}/{}, length: {:5.2f}, reward: {:5.2f}, training steps: {}\"\n",
    "          .format(e + 1, EPISODES, time_step, episode_reward, training_step))\n",
    "    t.refresh()\n",
    "    # Log the relevant parameters to the tensorboard.       \n",
    "    logger.log_scalar(\"Performance/EpisodeLength\", time_step, e)\n",
    "    logger.log_scalar(\"Performance/Reward\", episode_reward, e)\n",
    "    logger.log_running_average(\"Reward\", episode_reward)\n",
    "    logger.log_running_average(\"EpisodeLength\", time_step)\n",
    "    logger.log_scalar(\"Misc/Epsilon\", dqn_agent.epsilon, e)\n",
    "    logger.log_scalar(\"Misc/BufferLength\", len(replay_buffer),e)\n",
    "    \n",
    "    # Store the best model.\n",
    "    if(logger.get_running_average(\"Reward\") > best_avg_reward):\n",
    "        best_avg_reward = logger.get_running_average(\"Reward\")\n",
    "        dqn_agent.model.save(os.path.join(\"summaries\", logging_name, logging_name + \"_model.h5\"))\n",
    "        \n",
    "# NOTE: When the training has converged or the training takes too long it can be interrupted\n",
    "# at any time. The progress in form of a neural network model will still be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Let's test the network we trained on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/10, length: 278.00, reward: 279.00\n",
      "episode: 2/10, length: 269.00, reward: 270.00\n",
      "episode: 3/10, length: 214.00, reward: 215.00\n",
      "episode: 4/10, length: 348.00, reward: 349.00\n",
      "episode: 5/10, length: 303.00, reward: 304.00\n",
      "episode: 6/10, length: 216.00, reward: 217.00\n",
      "episode: 7/10, length: 332.00, reward: 333.00\n",
      "episode: 8/10, length: 252.00, reward: 253.00\n",
      "episode: 9/10, length: 219.00, reward: 220.00\n",
      "episode: 10/10, length: 228.00, reward: 229.00\n"
     ]
    }
   ],
   "source": [
    "# Load the Gym environment.\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create a DQN Agent instance.\n",
    "dqn_agent = DQNAgent(observation_shape=env.observation_space.shape, \n",
    "                     action_shape=env.action_space.n,\n",
    "                     epsilon=0.0)\n",
    "\n",
    "# TODO: Load your saved model\n",
    "dqn_agent.model = keras.models.load_model(os.path.join(\"summaries\", logging_name, logging_name + \"_model.h5\"))\n",
    "\n",
    "# Run episodes in the environment\n",
    "for e in range(10):\n",
    "    # Reset the environment and obtain the first observation\n",
    "    state = env.reset()\n",
    "\n",
    "    # Keep track of the summed reward\n",
    "    episode_reward = 0\n",
    "\n",
    "    for time_step in range(MAX_STEPS):\n",
    "        env.render()\n",
    "        # Choose an action.\n",
    "        action = dqn_agent.act(state)\n",
    "        # Perform the action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += np.sum(reward)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "\n",
    "        # If the episode is over break the loop and start with the next episode.\n",
    "        if done:\n",
    "            break\n",
    "                \n",
    "    print(\"episode: {}/{}, length: {:5.2f}, reward: {:5.2f}\"\n",
    "          .format(e + 1, 10, time_step, episode_reward))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
