{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data-science-blog.com/blog/2022/04/11/how-to-choose-the-best-pre-trained-model-for-your-convolutional-neural-network/\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fast_ml\n",
      "  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.Installing collected packages: fast-ml\n",
      "Successfully installed fast-ml-3.68\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install fast_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from efficientnet) (0.20.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from efficientnet) (1.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (2023.4.12)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (9.3.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (21.3)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (3.1)\n",
      "Requirement already satisfied: scipy<1.9.2,>=1.8 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (1.9.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from scikit-image->efficientnet) (2.27.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from packaging>=20.0->scikit-image->efficientnet) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U efficientnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "# from pyimagesearch.resnet import ResNet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "#from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from matplotlib import pyplot\n",
    "import efficientnet.keras as efn\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "#img = load_img('data/IMG_3559.jpg')\n",
    "\n",
    "def image_augmentation(data):\n",
    "    new_data = []\n",
    "    # convert to numpy array\n",
    "    for i in range(len(data)):\n",
    "        temp_data = img_to_array(data[i])\n",
    "        # normalize image\n",
    "        # print(\"data: \", data)\n",
    "        # normalize image\n",
    "        img_norm = cv2.normalize(temp_data, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "        # print(\"norm: \", img_norm)\n",
    "        # expand dimension to one sample\n",
    "        samples = expand_dims(img_norm, 0)\n",
    "        # create image data augmentation generator\n",
    "\n",
    "        \n",
    "        datagen = ImageDataGenerator(rescale = 1./255., width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = True ,rotation_range=30, brightness_range=[0.2,1.2], zoom_range=[0.6,1.1])\n",
    "        # prepare iterator\n",
    "        it = datagen.flow(samples, batch_size=1)\n",
    "        new_data.append(it)\n",
    "        \n",
    "        # generate samples and plot\n",
    "        for i in range(4):\n",
    "            # generate batch of images\n",
    "            batch = it.next()\n",
    "            # convert to unsigned integers for viewing\n",
    "            image = batch[0].astype('uint8')\n",
    "            im = array_to_img(image, scale=False)\n",
    "            im.save('/tmp/Datensatz/Datensatz/image_DA_'+str(i)+ '.jpg')\n",
    "\n",
    "            # pyplot.imshow(image)\n",
    "            # # show the figure\n",
    "            # pyplot.show()\n",
    "    #train_generator = datagen.flow_from_directory(new_data, batch_size = 20, class_mode = 'binary', target_size = (224, 224))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzipping data/Datensatz.zip\n",
      "/tmp/Datensatz/Datensatz/\n",
      "train    : 535 image paths\n"
     ]
    }
   ],
   "source": [
    "################ Unzip the dataset in the Colab runtime #################\n",
    "import glob\n",
    "import zipfile\n",
    "zip_file_path = \"data/Datensatz.zip\"\n",
    "\n",
    "pathToZip = os.path.join(zip_file_path)\n",
    "\n",
    "print(\"unzipping {}\".format(pathToZip))\n",
    "\n",
    "pathToData = \"/tmp/Datensatz\"\n",
    "with zipfile.ZipFile(pathToZip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(pathToData)\n",
    "\n",
    "# training images\n",
    "data    = glob.glob(os.path.join(pathToData, \"Datensatz/\" + \"*.jpeg\"))\n",
    "# test images\n",
    "#test    = glob.glob(os.path.join(pathToData, \"Datensatz/test/\" + \"*.jpg\"))\n",
    "\n",
    "print(pathToData + \"/Datensatz/\") #/train\n",
    "print(\"train    : {} image paths\".format(len(data)))\n",
    "#print(\"train    : {} image paths\".format(len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list:  535\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2316x2316 at 0x1624602CD30>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "image_list = []\n",
    "for filename in glob.glob(\"/tmp/Datensatz/Datensatz/\" + \"*.jpeg\"): #assuming gif\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(im)\n",
    "\n",
    "print(\"list: \", len(image_list))\n",
    "print(image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 105. MiB for an array with shape (3024, 3024, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\efficientNet.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m samples \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(image_list)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         temp_data \u001b[39m=\u001b[39m img_to_array(image_list[i])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# normalize image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# print(\"data: \", data)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# normalize image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         img_norm \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mnormalize(temp_data, \u001b[39mNone\u001b[39;00m, alpha\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, beta\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, norm_type\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mNORM_MINMAX, dtype\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mCV_32F)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py:228\u001b[0m, in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    226\u001b[0m     dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[0;32m    227\u001b[0m   kwargs[\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dtype\n\u001b[1;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39mimg_to_array(img, data_format\u001b[39m=\u001b[39mdata_format, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:309\u001b[0m, in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnknown data_format: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m data_format)\n\u001b[0;32m    306\u001b[0m \u001b[39m# Numpy array x has format (height, width, channel)\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# or (channel, height, width)\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m# but original PIL image has format (width, height, channel)\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(img, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m data_format \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 105. MiB for an array with shape (3024, 3024, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "samples = []\n",
    "for i in range(len(image_list)):\n",
    "        temp_data = img_to_array(image_list[i])\n",
    "        # normalize image\n",
    "        # print(\"data: \", data)\n",
    "        # normalize image\n",
    "        img_norm = cv2.normalize(temp_data, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "        # print(\"norm: \", img_norm)\n",
    "        # expand dimension to one sample\n",
    "        samples.append(expand_dims(img_norm.astype(np.uint8), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=None and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\efficientNet.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     y\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mDominik\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# In the first step we will split the data in training and remaining dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m X_train, X_rem, y_train, y_rem \u001b[39m=\u001b[39m train_test_split(samples,y, train_size\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Now since we want the valid and test size to be equal (10% each of overall data). \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# we have to define valid_size=0.5 (that is 50% of remaining data)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m test_size \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=None and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's say we want to split the data in 80:10:10 for train:valid:test dataset\n",
    "train_size=0.8\n",
    "y = []\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    y.append('Dominik')\n",
    "\n",
    "\n",
    "# In the first step we will split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(samples,y, train_size=0.8)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "print(X_train), print(y_train)\n",
    "print(X_valid), print(y_valid)\n",
    "print(X_test), print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(316, len(X_train)):\n",
    "    X_train[i].save(r'C:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\data\\data\\Dominik\\train\\image_' +str(i) +'.jpg', 'JPEG')\n",
    "    \n",
    "for i in range(len(X_valid)):\n",
    "    X_train[i].save(r'C:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\data\\data\\Dominik\\validation\\image_' +str(i) +'.jpg', 'JPEG')\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_train[i].save(r'C:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\data\\data\\Dominik\\test\\image_' +str(i) +'.jpg', 'JPEG')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_path = pd.DataFrame (X_train, columns = ['image_path'])\n",
    "df_valid_path = pd.DataFrame (X_valid, columns = ['image_path'])\n",
    "df_test_path = pd.DataFrame (X_test, columns = ['image_path'])\n",
    "\n",
    "df_train_label = pd.DataFrame (y_train, columns = ['label'])\n",
    "df_valid_label = pd.DataFrame (y_valid, columns = ['label'])\n",
    "df_test_label = pd.DataFrame (y_test, columns = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'JpegImageFile' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\efficientNet.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m path_test \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tmp/face_images/face_images/test\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# train_generator = train_datagen.flow_from_directory(path_train, batch_size = 20, class_mode = 'binary', target_size = (224, 224), save_to_dir= 'data/new2/')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# test_generator = test_datagen.flow_from_directory(path_test, batch_size = 20, class_mode = 'binary', target_size = (224, 224), save_to_dir= 'data/new2/')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_generator \u001b[39m=\u001b[39m train_datagen\u001b[39m.\u001b[39;49mflow(x\u001b[39m=\u001b[39;49mX_train, y\u001b[39m=\u001b[39;49my_train, batch_size \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m, save_to_dir\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdata/split_data/train/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m test_generator \u001b[39m=\u001b[39m test_datagen\u001b[39m.\u001b[39mflow(x\u001b[39m=\u001b[39mdf_valid_path,y\u001b[39m=\u001b[39mdf_valid_label, batch_size \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m, save_to_dir\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/split_data/validation/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_generator \u001b[39m=\u001b[39m test_datagen\u001b[39m.\u001b[39mflow(x\u001b[39m=\u001b[39mdf_test_path,y\u001b[39m=\u001b[39mdf_test_label, batch_size \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m, save_to_dir\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/split_data/test/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py:866\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    816\u001b[0m          x,\n\u001b[0;32m    817\u001b[0m          y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m          save_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpng\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    825\u001b[0m          subset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    826\u001b[0m   \u001b[39m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \n\u001b[0;32m    828\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m \n\u001b[0;32m    865\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 866\u001b[0m   \u001b[39mreturn\u001b[39;00m NumpyArrayIterator(\n\u001b[0;32m    867\u001b[0m       x,\n\u001b[0;32m    868\u001b[0m       y,\n\u001b[0;32m    869\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    870\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    871\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    872\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    873\u001b[0m       seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    874\u001b[0m       data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[0;32m    875\u001b[0m       save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[0;32m    876\u001b[0m       save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[0;32m    877\u001b[0m       save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[0;32m    878\u001b[0m       subset\u001b[39m=\u001b[39;49msubset)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py:445\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[0;32m    443\u001b[0m     dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[0;32m    444\u001b[0m   kwargs[\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dtype\n\u001b[1;32m--> 445\u001b[0m \u001b[39msuper\u001b[39m(NumpyArrayIterator, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    446\u001b[0m     x, y, image_data_generator,\n\u001b[0;32m    447\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    448\u001b[0m     shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[0;32m    449\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    450\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[0;32m    451\u001b[0m     data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[0;32m    452\u001b[0m     save_to_dir\u001b[39m=\u001b[39msave_to_dir,\n\u001b[0;32m    453\u001b[0m     save_prefix\u001b[39m=\u001b[39msave_prefix,\n\u001b[0;32m    454\u001b[0m     save_format\u001b[39m=\u001b[39msave_format,\n\u001b[0;32m    455\u001b[0m     subset\u001b[39m=\u001b[39msubset,\n\u001b[0;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:76\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[0;32m     74\u001b[0m     x \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m xx \u001b[39min\u001b[39;00m x_misc:\n\u001b[1;32m---> 76\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(x) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(xx):\n\u001b[0;32m     77\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mAll of the arrays in `x` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     79\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mshould have the same length. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     80\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mFound a pair with: len(x[0]) = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, len(x[?]) = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     81\u001b[0m                 (\u001b[39mlen\u001b[39m(x), \u001b[39mlen\u001b[39m(xx)))\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'JpegImageFile' has no len()"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255., width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = True ,rotation_range=70, brightness_range=[0.2,1.2], zoom_range=[0.6,1.1], fill_mode='reflect')\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "path_train = \"/tmp/face_images/face_images/train\"\n",
    "path_test = \"/tmp/face_images/face_images/test\"\n",
    "# train_generator = train_datagen.flow_from_directory(path_train, batch_size = 20, class_mode = 'binary', target_size = (224, 224), save_to_dir= 'data/new2/')\n",
    "# test_generator = test_datagen.flow_from_directory(path_test, batch_size = 20, class_mode = 'binary', target_size = (224, 224), save_to_dir= 'data/new2/')\n",
    "\n",
    "train_generator = train_datagen.flow(x=X_train, y=y_train, batch_size = 20, save_to_dir= 'data/split_data/train/')\n",
    "test_generator = test_datagen.flow(x=df_valid_path,y=df_valid_label, batch_size = 20, save_to_dir= 'data/split_data/validation/')\n",
    "test_generator = test_datagen.flow(x=df_test_path,y=df_test_label, batch_size = 20, save_to_dir= 'data/split_data/test/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Loading the Base Model\n",
    "# sampleImage = []\n",
    "# for i in range(len(train)):\n",
    "#     sampleImage.append(cv2.imread(train[i]))\n",
    "#print(\"sampleImage: \", sampleImage[2])\n",
    "#train_dat, train_generator = image_augmentation(sampleImage)\n",
    "\n",
    "base_model = efn.EfficientNetB7(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, let us freeze the layers:\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build the model\n",
    "\n",
    "x = base_model.output\n",
    "y = base_model.input\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "model_final = Model(y, predictions) #base_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Compile and Fit\n",
    "\n",
    "model_final.compile(optimizers.RMSprop(lr=0.0001, decay=1e-6),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-Size: Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the    predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient.\n",
    "-What if the dataset does not divide evenly by the batch size?\n",
    "    --> This can and does happen often when training a model. It simply means that the final batch has fewer samples than the other batches.\n",
    "\n",
    "Epochen: The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "\n",
    "One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. For example, as above, an epoch that has one batch is called the batch gradient descent learning algorithm.\n",
    "\n",
    "You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.\n",
    "\n",
    "The batch size is a number of samples processed before the model is updated.\n",
    "\n",
    "The number of epochs is the number of complete passes through the training dataset.\n",
    "\n",
    "\n",
    "Steps_per_epoch: Total number of steps (batches of samples)\n",
    "            before declaring one epoch finished and starting the\n",
    "            next epoch. When training with input tensors such as\n",
    "            TensorFlow data tensors, the default `None` is equal to\n",
    "            the number of samples in your dataset divided by\n",
    "            the batch size, or 1 if that cannot be determined. If x is a\n",
    "            `tf.data` dataset, and 'steps_per_epoch'\n",
    "            is None, the epoch will run until the input dataset is exhausted.\n",
    "            When passing an infinitely repeating dataset, you must specify the\n",
    "            `steps_per_epoch` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\domin\\Documents\\Studium\\Master\\Deep-Learning\\Projekt\\face_recognition\\efficientNet.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We finally fit the model on our data:\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/domin/Documents/Studium/Master/Deep-Learning/Projekt/face_recognition/efficientNet.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m eff_history \u001b[39m=\u001b[39m model_final\u001b[39m.\u001b[39;49mfit(train_generator, validation_data \u001b[39m=\u001b[39;49m train_generator, steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1177\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1178\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1179\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1180\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1181\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1182\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1183\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1184\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1185\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# We finally fit the model on our data:\n",
    "\n",
    "eff_history = model_final.fit(train_generator, validation_data = train_generator, steps_per_epoch = 2, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OpenCV package\n",
    "import cv2\n",
    "  \n",
    "# Reading the image\n",
    "img = cv2.imread('Photos/cric4.jpg')\n",
    "  \n",
    "# Converting image to grayscale\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "# Loading the required haar-cascade xml classifier file\n",
    "haar_cascade = cv2.CascadeClassifier('Haarcascade_frontalface_default.xml')\n",
    "  \n",
    "# Applying the face detection method on the grayscale image\n",
    "faces_rect = haar_cascade.detectMultiScale(gray_img, 1.1, 9)\n",
    "  \n",
    "# Iterating through rectangles of detected faces\n",
    "for (x, y, w, h) in faces_rect:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "  \n",
    "cv2.imshow('Detected faces', img)\n",
    "  \n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
